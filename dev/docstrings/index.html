<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Docstrings · TensorCast</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">TensorCast</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../basics/">Basics</a></li><li><a class="tocitem" href="../reduce/">Reduction</a></li><li><a class="tocitem" href="../options/">Options</a></li><li class="is-active"><a class="tocitem" href>Docstrings</a><ul class="internal"><li><a class="tocitem" href="#String-macros-1"><span>String macros</span></a></li><li class="toplevel"><a class="tocitem" href="#Functions-1"><span>Functions</span></a></li><li><a class="tocitem" href="#Reshaping-and-views-1"><span>Reshaping &amp; views</span></a></li><li><a class="tocitem" href="#Slicing-and-glueing-1"><span>Slicing &amp; glueing</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Docstrings</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Docstrings</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/mcabbott/TensorCast.jl/blob/master/docs/src/docstrings.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Macros-1"><a class="docs-heading-anchor" href="#Macros-1">Macros</a><a class="docs-heading-anchor-permalink" href="#Macros-1" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="TensorCast.@cast-Tuple{Any}" href="#TensorCast.@cast-Tuple{Any}"><code>TensorCast.@cast</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia">@cast Z[i,j,...] := f(A[i,j,...], B[j,k,...])  options</code></pre><p>Macro for broadcasting, reshaping, and slicing of arrays in index notation. Understands the following things:</p><ul><li><code>A[i,j,k]</code> is a three-tensor with these indices.</li><li><code>B[(i,j),k]</code> is the same thing, reshaped to a matrix. Its first axis (the bracket) is indexed by <code>n = i + (j-1) * N</code> where <code>i ∈ 1:N</code>. This may also be written <code>B[i\j,k]</code> or <code>B[i⊗j,k]</code>.</li><li><code>C[k][i,j]</code> is a vector of matrices, either created by slicing (if on the left) or implying glueing into (if on the right) a 3-tensor <code>A[i,j,k]</code>.</li><li><code>D[j,k]{i}</code> is an ordinary matrix of <code>SVector</code>s, which may be reinterpreted from <code>A[i,j,k]</code>.</li><li><code>E[i,_,k]</code> has two nontrivial dimensions, and <code>size(E,2)==1</code>. On the right hand side (or when writing to an existing array) you may also write <code>E[i,3,k]</code> meaning <code>view(E, :,3,:)</code>, or <code>E[i,$c,j]</code> to use a variable <code>c</code>.</li><li><code>f(x)[i,j,k]</code> is allowed; <code>f(x)</code> must return a 3-tensor (and will be evaluated only once).</li><li><code>g(H[:,k])[i,j]</code> is a generalised <code>mapslices</code>, with <code>g</code> mapping columns of <code>H</code>   to matrices, which are glued into a 3-tensor <code>A[i,j,k]</code>.</li><li><code>h(I[i], J[j])[k]</code> expects an <code>h</code> which maps two scalars to a vector, which gets broadcasted <code>h.(I,J&#39;)</code>, then glued to make a 3-tensor.</li><li><code>K[i,j]&#39;</code> conjugates each element, equivalent to <code>K&#39;[j,i]</code> which is the conjugate-transpose of the matrix.</li><li><code>M[i,i]</code> means <code>diag(M)[i]</code>, but only for matrices: <code>N[i,i,k]</code> is an error.</li><li><code>P[i,i&#39;]</code> is normalised to <code>P[i,i′]</code> with unicode \prime.</li><li><code>R[i,-j,k]</code> means roughly <code>reverse(R, dims=2)</code>, and <code>Q[i,~j,k]</code> similar with <code>shuffle</code>.</li><li><code>S[i,T[j,k]]</code> is the 3-tensor <code>S[:,T]</code> created by indexing a matrix <code>S</code> with <code>T</code>, where these integers are <code>all(1 .&lt;= T .&lt;= size(S,2))</code>.</li></ul><p>The left and right hand sides must have all the same indices, and the only repeated index allowed is <code>M[i,i]</code>, which is a diagonal not a trace. See <code>@reduce</code> and <code>@matmul</code> for related macros which can sum over things.</p><p>If a function of one or more tensors appears on the right hand side, then this represents a broadcasting operation, and the necessary re-orientations of axes are automatically inserted.</p><p>The following actions are possible:</p><ul><li><code>=</code> writes into an existing array, overwriting its contents, while <code>+=</code> adds (precisely <code>Z .= Z .+ ...</code>) and <code>*=</code> multiplies.</li><li><code>:=</code> creates a new array. This need not be named: <code>Z = @cast [i,j,k] := ...</code> is allowed.</li><li><code>|=</code> insists that this is a copy, not a view.</li></ul><p>Re-ordering of indices <code>Z[k,j,i]</code> is done lazily with <code>PermutedDimsArray(A, ...)</code>. Reversing of an axis <code>F[i,-j,k]</code> is also done lazily, by <code>Reverse{2}(F)</code> which makes a <code>view</code>. Using <code>|=</code> (or broadcasting) will produce a simple <code>Array</code>.</p><p>Options can be specified at the end (if several, separated by <code>,</code> i.e. <code>options::Tuple</code>)</p><ul><li><code>i:3</code> supplies the range of index <code>i</code>. Variables and functions like <code>j:Nj, k:length(K)</code> are allowed.</li><li><code>assert</code> will turn on explicit dimension checks of the input. (Providing any ranges will also turn these on.)</li><li><code>cat</code> will glue slices by things like <code>hcat(A...)</code> instead of the default <code>reduce(hcat, A)</code>, and <code>lazy</code> will instead make a <code>LazyStack.Stacked</code> container.</li><li><code>nolazy</code> disables <code>PermutedDimsArray</code> and <code>Reverse</code> in favour of <code>permutedims</code> and <code>reverse</code>, and <code>Diagonal</code> in favour of <code>diagm</code> for <code>Z[i,i]</code> output.</li><li><code>strided</code> will place <code>@strided</code> in front of broadcasting operations, and use <code>@strided permutedims(A, ...)</code> instead of <code>PermutedDimsArray(A, ...)</code>. For this you need <code>using Strided</code> to load that package.</li><li><code>avx</code> will place <code>@avx</code> in front of broadcasting operations, which needs <code>using LoopVectorization</code> to load that package.</li></ul><p>To create static slices <code>D[k]{i,j}</code> you should give all slice dimensions explicitly. You may write <code>D[k]{i:2,j:2}</code> to specify <code>Size(2,2)</code> slices. They are made most cleanly from the first indices of the input, i.e. this <code>D</code> from <code>A[i,j,k]</code>. The notation <code>A{:,:,k}</code> will only work in this order, and writing <code>A{:2,:2,k}</code> provides the sizes.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mcabbott/TensorCast.jl/blob/c72e4f63f3730fef9f384520b99ea5df18a026ce/src/macro.jl#L15-L80">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="TensorCast.@reduce-Tuple{Any}" href="#TensorCast.@reduce-Tuple{Any}"><code>TensorCast.@reduce</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia">@reduce A[i] := sum(j,k) B[i,j,k]             # A = vec(sum(B, dims=(2,3)))
@reduce A[i] := prod(j) B[i] + ε * C[i,j]     # A = vec(prod(B .+ ε .* C, dims=2))
@reduce A[i] = sum(j) exp( C[i,j] / D[j] )    # sum!(A, exp.(C ./ D&#39;) )</code></pre><p>Tensor reduction macro:</p><ul><li>The reduction function can be anything which works like <code>sum(B, dims=(1,3))</code>, for instance <code>prod</code> and <code>maximum</code> and <code>Statistics.mean</code>.</li><li>In-place operations <code>Z[j] = sum(...</code> will construct the banged version of the given function&#39;s name, which must work like <code>sum!(Z, A)</code>.</li><li>The tensors can be anything that <code>@cast</code> understands, including gluing of slices <code>B[i,k][j]</code> and reshaping <code>B[i\j,k]</code>. See <code>? @cast</code> for the complete list.</li><li>If there is a function of one or more tensors on the right, then this is a broadcasting operation.</li><li>Index ranges may be given afterwards (as for <code>@cast</code>) or inside the reduction <code>sum(i:3, k:4)</code>.</li><li>All indices appearing on the right must appear either within <code>sum(...)</code> etc, or on the left.</li></ul><pre><code class="language-none">F = @reduce sum(i,j)  B[i] + γ * D[j]         # sum(B .+ γ .* D&#39;)
@reduce G[] := sum(i,j)  B[i] + γ * D[j]      # F == G[]</code></pre><p>Complete reduction to a scalar output <code>F</code>, or a zero-dim array <code>G</code>. <code>G[]</code> involves <code>sum(A, dims=(1,2))</code> rather than <code>sum(A)</code>.</p><pre><code class="language-none">@reduce Z[k] := sum(i,j) A[i] * B[j] * C[k]  lazy, i:N, j:N, k:N</code></pre><p>The option <code>lazy</code> replaces the broadcast expression with a <code>BroadcastArray</code>, to avoid <code>materialize</code>ing the entire array before summing. In the example this is of size <code>N^3</code>.</p><p>The option <code>strided</code> will place <code>@strided</code> in front of the broadcasting operation. You need <code>using Strided</code> for this to work.</p><pre><code class="language-none">@reduce sum(i) A[i] * log(@reduce [i] := sum(j) A[j] * exp(B[i,j]))
@cast W[i] := A[i] * exp(- @reduce S[i] = sum(j) exp(B[i,j]) lazy)</code></pre><p>Recursion like this is allowed, inside either <code>@cast</code> or <code>@reduce</code>. The intermediate array need not have a name, like <code>[i]</code>, unless writing into an existing array, like <code>S[i]</code> here.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mcabbott/TensorCast.jl/blob/c72e4f63f3730fef9f384520b99ea5df18a026ce/src/macro.jl#L86-L124">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="TensorCast.@matmul-Tuple{Any}" href="#TensorCast.@matmul-Tuple{Any}"><code>TensorCast.@matmul</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia">@matmul M[a,c] := sum(b)  A[a,b] * B[b,c]</code></pre><p>Matrix multiplication macro. Uses the same syntax as <code>@reduce</code>, but instead of broadcasting the expression on the right out to a 3-tensor before summing it along one dimension, this calls <code>*</code> or <code>mul!</code>, which is usually much faster. But it only works on expressions of suitable form.</p><p>With more than two tensors on the right, it proceeds left to right, and each summed index must appear on two tensors, probably neighbours.</p><p>Note that unlike <code>@einsum</code> and <code>@tensor</code>, you must explicitly specify what indices to sum over. Both this macro and <code>@reduce</code> could in principal infer this, and perhaps I will add that later... but then I find myself commenting <code># sum_μ</code> anyway.</p><pre><code class="language-none">@matmul Z[a⊗b,z] = sum(i,j,k)  D[i,a][j] * E[i⊗j,_,k,b] * F[z,3,k]</code></pre><p>Each tensor will be pre-processed exactly as for <code>@cast</code> / <code>@reduce</code>, here glueing slices of <code>D</code> together, reshaping <code>E</code>, and taking a view of <code>F</code>. Once this is done, the right hand side must be of the form <code>(tensor) * (tensor) * ...</code>, which becomes <code>mul!(ZZ, (DD * EE), FF)</code>.</p><pre><code class="language-none">@reduce V[i] := sum(k) W[k] * exp(@matmul [i,k] := sum(j) A[i,j] * B[j,k])</code></pre><p>You should be able to use this within the other macros, as shown.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mcabbott/TensorCast.jl/blob/c72e4f63f3730fef9f384520b99ea5df18a026ce/src/macro.jl#L131-L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="TensorCast.@pretty-Tuple{Any}" href="#TensorCast.@pretty-Tuple{Any}"><code>TensorCast.@pretty</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia">@pretty @cast A[...] := B[...]</code></pre><p>Prints an approximately equivalent expression with the macro expanded. Compared to <code>@macroexpand1</code>, generated symbols are replaced with animal names (from MacroTools), comments are deleted, module names are removed from functions, and the final expression is fed to <code>println()</code>.</p><p>To copy and run the printed expression, you may need various functions which aren&#39;t exported. Try something like <code>using TensorCast: orient, star, rview, @assert_, red_glue, sliceview</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mcabbott/TensorCast.jl/blob/c72e4f63f3730fef9f384520b99ea5df18a026ce/src/pretty.jl#L2-L12">source</a></section></article><h2 id="String-macros-1"><a class="docs-heading-anchor" href="#String-macros-1">String macros</a><a class="docs-heading-anchor-permalink" href="#String-macros-1" title="Permalink"></a></h2><p>These provide an alternative... but don&#39;t cover quite everything:</p><article class="docstring"><header><a class="docstring-binding" id="TensorCast.@cast_str" href="#TensorCast.@cast_str"><code>TensorCast.@cast_str</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia">cast&quot; Z_ij := A_i + B_j &quot;</code></pre><p>String macro version of <code>@cast</code>, which translates things like <code>&quot;A_ijk&quot; == A[i,j,k]</code>. Indices should be single letters, except for primes, for example:</p><pre><code class="language-none">julia&gt; @pretty cast&quot; X_αβ&#39; = A_α3β&#39; * log(B_β&#39;$k)&quot;
# @cast  X[α,β′] = A[α,3,β′] * log(B[β′,$k])
begin
    local kangaroo = view(A, :, 3, :)
    local turtle = transpose(view(B, :, k))
    X .= @. kangaroo * log(turtle)
end</code></pre><p>Underscores (trivial dimensions) and colons (for mapslices) are also understood:</p><pre><code class="language-none">julia&gt; @pretty cast&quot; Y_ijk := f(A_j:k)_i + B_i_k + (C_k)_i&quot;
# @cast  Y[i,j,k] := f(A[j,:,k])[i] + B[i,_,k] + C[k][i]</code></pre><p>Operators <code>:=</code> and <code>=</code> work as usual, as do options. There are similar macros <code>reduce&quot;Z_i = Σ_j A_ij&quot;</code> and <code>matmul&quot;Z_ik = Σ_j A_ij * B_jk&quot;</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mcabbott/TensorCast.jl/blob/c72e4f63f3730fef9f384520b99ea5df18a026ce/src/string.jl#L3-L24">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="TensorCast.@reduce_str" href="#TensorCast.@reduce_str"><code>TensorCast.@reduce_str</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia">reduce&quot; Z_i := sum_j A_ij + B_j &quot;</code></pre><p>String macro version of <code>@reduce</code>. Indices should be single letters, except for primes, and constants. You may write \Sigma <code>Σ</code> for <code>sum</code> (and \Pi <code>Π</code> for <code>prod</code>):</p><pre><code class="language-none">julia&gt; @pretty reduce&quot; W_i = Σ_i&#39; A_ii&#39; / B_i&#39;3^2  lazy&quot;
# @reduce  W[i] = sum(i′) A[i,i′] / B[i′,3]^2  lazy
begin
    local mallard = orient(view(B, :, 3), (*, :))
    sum!(W, @__dot__(lazy(A / mallard ^ 2)))
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mcabbott/TensorCast.jl/blob/c72e4f63f3730fef9f384520b99ea5df18a026ce/src/string.jl#L35-L49">source</a></section></article><h1 id="Functions-1"><a class="docs-heading-anchor" href="#Functions-1">Functions</a><a class="docs-heading-anchor-permalink" href="#Functions-1" title="Permalink"></a></h1><p>These are not exported, but are called by the macros above,  and visible in what <code>@pretty</code> prints out. </p><h2 id="Reshaping-and-views-1"><a class="docs-heading-anchor" href="#Reshaping-and-views-1">Reshaping &amp; views</a><a class="docs-heading-anchor-permalink" href="#Reshaping-and-views-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="TensorCast.Fast.orient" href="#TensorCast.Fast.orient"><code>TensorCast.Fast.orient</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">B = orient(A, code)</code></pre><p>Usually this calls <code>_orient(A, code)</code>, which reshapes <code>A</code> such that its nontrivial axes lie in the directions where <code>code</code> contains a <code>:</code>, by inserting axes on which <code>size(B, d) == 1</code> as needed.</p><p>When acting on <code>A::Transpose</code>, <code>A::PermutedDimsArray</code> etc, it will <code>collect(A)</code> first, because reshaping these is very slow. However, this is only done when the underlying array is a &quot;normal&quot; CPU <code>Array</code>, since e.g., for GPU arrays, <code>collect(A)</code> copies the array to the CPU. Fortunately, the speed penalty for reshaping transposed GPU arrays is lower than on the CPU.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mcabbott/TensorCast.jl/blob/c72e4f63f3730fef9f384520b99ea5df18a026ce/src/view.jl#L23-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="TensorCast.Fast.PermuteDims" href="#TensorCast.Fast.PermuteDims"><code>TensorCast.Fast.PermuteDims</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">PermuteDims(A::Matrix)
PermuteDims(A::Vector)</code></pre><p>Lazy like <code>transpose</code>, but not recursive: calls <code>PermutedDimsArray</code> unless <code>eltype(A) &lt;: Number</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mcabbott/TensorCast.jl/blob/c72e4f63f3730fef9f384520b99ea5df18a026ce/src/view.jl#L179-L185">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="TensorCast.Fast.rview" href="#TensorCast.Fast.rview"><code>TensorCast.Fast.rview</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">rview(A, :,1,:) ≈ (@assert size(A,2)==1; view(A, :,1,:))</code></pre><p>This simply reshapes <code>A</code> so as to remove a trivial dimension where indicated. Throws an error if size of <code>A</code> is not 1 in the indicated dimension.</p><p>Will fail silently if given a <code>(:,3,:)</code> or <code>(:,$c,:)</code>, for which <code>needview!(code) = true</code>, so the macro should catch such cases.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mcabbott/TensorCast.jl/blob/c72e4f63f3730fef9f384520b99ea5df18a026ce/src/view.jl#L112-L120">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="TensorCast.Fast.diagview" href="#TensorCast.Fast.diagview"><code>TensorCast.Fast.diagview</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">diagview(M) = view(M, diagind(M))</code></pre><p>Like <code>diag(M)</code> but makes a view.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mcabbott/TensorCast.jl/blob/c72e4f63f3730fef9f384520b99ea5df18a026ce/src/view.jl#L2-L6">source</a></section></article><h2 id="Slicing-and-glueing-1"><a class="docs-heading-anchor" href="#Slicing-and-glueing-1">Slicing &amp; glueing</a><a class="docs-heading-anchor-permalink" href="#Slicing-and-glueing-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="TensorCast.Fast.sliceview" href="#TensorCast.Fast.sliceview"><code>TensorCast.Fast.sliceview</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">sliceview(A, code)
slicecopy(A, code)</code></pre><p>Slice array <code>A</code> according to <code>code</code>, a tuple of length <code>ndims(A)</code>, in which <code>:</code> indicates a dimension of the slices, and <code>*</code> a dimension separating them. For example if <code>code = (:,*,:)</code> then slices are either <code>view(A, :,i,:)</code> or <code>A[:,i,:]</code> with <code>i=1:size(A,2)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mcabbott/TensorCast.jl/blob/c72e4f63f3730fef9f384520b99ea5df18a026ce/src/slice.jl#L2-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="TensorCast.Fast.glue" href="#TensorCast.Fast.glue"><code>TensorCast.Fast.glue</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">glue!(B, A, code)
copy_glue(A, code) = glue!(Array{T}(...), A, code)</code></pre><p>Copy the contents of an array of arrays into one larger array, un-doing <code>sliceview</code> / <code>slicecopy</code> with the same <code>code</code>. This function can handle arbitrary codes. (Also called <code>stack</code> or <code>align</code> elsewhere.)</p><pre><code class="language-none">cat_glue(A, code)
red_glue(A, code)</code></pre><p>The same result, but calling either things like <code>hcat(A...)</code> or things like <code>reduce(hcat, A)</code>. The code must be sorted like <code>(:,:,:,*,*)</code>, except that <code>(*,:)</code> is allowed.</p><pre><code class="language-none">glue(A)
glue(A, code)</code></pre><p>If <code>code</code> is omitted, the default is like <code>(:,:,*,*)</code> with <code>ndims(first(A))</code> colons first, then <code>ndims(A)</code> stars. If the inner arrays are <code>StaticArray</code>s (and the code is sorted) then it calls <code>static_glue</code>. Otherwise it will call <code>red_glue</code>, unless code is unsuitable for that, in which case <code>copy_glue</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mcabbott/TensorCast.jl/blob/c72e4f63f3730fef9f384520b99ea5df18a026ce/src/slice.jl#L35-L58">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../options/">« Options</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 16 May 2020 09:42">Saturday 16 May 2020</span>. Using Julia version 1.3.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
