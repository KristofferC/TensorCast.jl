<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Docstrings · TensorCast</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">TensorCast</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../basics/">Basics</a></li><li><a class="tocitem" href="../reduce/">Reduction</a></li><li><a class="tocitem" href="../options/">Options</a></li><li class="is-active"><a class="tocitem" href>Docstrings</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Functions"><span>Functions</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Docstrings</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Docstrings</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/mcabbott/TensorCast.jl/blob/master/docs/src/docstrings.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Macros"><a class="docs-heading-anchor" href="#Macros">Macros</a><a id="Macros-1"></a><a class="docs-heading-anchor-permalink" href="#Macros" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="TensorCast.@cast-Tuple{Any}" href="#TensorCast.@cast-Tuple{Any}"><code>TensorCast.@cast</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia">@cast Z[i,j,...] := f(A[i,j,...], B[j,k,...])  options</code></pre><p>Macro for broadcasting, reshaping, and slicing of arrays in index notation. Understands the following things:</p><ul><li><code>A[i,j,k]</code> is a three-tensor with these indices.</li><li><code>B[(i,j),k]</code> is the same thing, reshaped to a matrix. Its first axis (the bracket) is indexed by <code>n = i + (j-1) * N</code> where <code>i ∈ 1:N</code>. This may also be written <code>B[i⊗j,k]</code>.</li><li><code>C[k][i,j]</code> is a vector of matrices, either created by slicing (if on the left) or implying glueing into (if on the right) a 3-tensor <code>A[i,j,k]</code>.</li><li><code>D[j,k]{i}</code> is an ordinary matrix of <code>SVector</code>s, which may be reinterpreted from <code>A[i,j,k]</code>.</li><li><code>E[i,_,k]</code> has two nontrivial dimensions, and <code>size(E,2)==1</code>. On the right hand side (or when writing to an existing array) you may also write <code>E[i,3,k]</code> meaning <code>view(E, :,3,:)</code>, or <code>E[i,$c,j]</code> to use a variable <code>c</code>.</li><li><code>f(x)[i,j,k]</code> is allowed; <code>f(x)</code> must return a 3-tensor (and will be evaluated only once).</li><li><code>g(H[:,k])[i,j]</code> is a generalised <code>mapslices</code>, with <code>g</code> mapping columns of <code>H</code>   to matrices, which are glued into a 3-tensor <code>A[i,j,k]</code>.</li><li><code>h(I[i], J[j])[k]</code> expects an <code>h</code> which maps two scalars to a vector, which gets broadcasted <code>h.(I,J&#39;)</code>, then glued to make a 3-tensor.</li><li><code>K[i,j]&#39;</code> conjugates each element, equivalent to <code>K&#39;[j,i]</code> which is the conjugate-transpose of the matrix.</li><li><code>M[i,i]</code> means <code>diag(M)[i]</code>, but only for matrices: <code>N[i,i,k]</code> is an error.</li><li><code>P[i,i&#39;]</code> is normalised to <code>P[i,i′]</code> with unicode \prime.</li><li><code>R[i,-j,k]</code> means roughly <code>reverse(R, dims=2)</code>, and <code>Q[i,~j,k]</code> similar with <code>shuffle</code>.</li><li><code>S[i,T[j,k]]</code> is the 3-tensor <code>S[:,T]</code> created by indexing a matrix <code>S</code> with <code>T</code>, where these integers are <code>all(1 .&lt;= T .&lt;= size(S,2))</code>.</li></ul><p>The left and right hand sides must have all the same indices, and the only repeated index allowed is <code>M[i,i]</code>, which is a diagonal not a trace. See <code>@reduce</code> and <code>@matmul</code> for related macros which can sum over things.</p><p>If a function of one or more tensors appears on the right hand side, then this represents a broadcasting operation, and the necessary re-orientations of axes are automatically inserted.</p><p>The following actions are possible:</p><ul><li><code>=</code> writes into an existing array, overwriting its contents, while <code>+=</code> adds (precisely <code>Z .= Z .+ ...</code>) and <code>*=</code> multiplies.</li><li><code>:=</code> creates a new array. To omit the name, write <code>Z = @cast _[i,j,k] := ...</code>.</li><li><code>|=</code> insists that the result is an <code>Array</code> not a view, or some other lazy wapper. (This may still be a <code>reshape</code> of the input, it does not guarantee a copy.)</li></ul><p>Options specified at the end (if several, separated by <code>,</code>) are:</p><ul><li><code>i in 1:3</code> or <code>i ∈ 1:3</code> supplies the range of index <code>i</code>. Variables and functions like <code>j in 1:Nj, k in 1:length(K)</code> are allowed, but <code>i = 1:3</code> is not.</li><li><code>lazy=false</code> disables <code>PermutedDimsArray</code> in favour of <code>permutedims</code>,  and <code>Diagonal</code> in favour of <code>diagm</code> for <code>Z[i,i]</code> output.</li></ul><p>Some modifications to broadcasting are possible, after loading the corresponding package:</p><ul><li><code>@cast @strided Z[i,j] := ...</code> uses Strided.jl&#39;s macro, for multi-threaded broadcasting.</li><li><code>@cast @avx Z[i,j] := ...</code> uses LoopVectorization.jl&#39;s macro, for SIMD acceleration.</li><li><code>@cast @lazy Z[i,j] := ...</code> uses LazyArrays.jl&#39;s BroadcastArray type, although there is no such macro.</li></ul><p>To create static slices <code>D[k]{i,j}</code> you should give all slice dimensions explicitly. You may write <code>D[k]{i:2,j:2}</code> to specify <code>Size(2,2)</code> slices. They are made most cleanly from the first indices of the input, i.e. this <code>D</code> from <code>A[i,j,k]</code>. The notation <code>A{:,:,k}</code> will only work in this order, and writing <code>A{:2,:2,k}</code> provides the sizes.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mcabbott/TensorCast.jl/blob/f29d3902927a900fdf692a2dadcf1f98f2948d68/src/macro.jl#L15-L73">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="TensorCast.@reduce-Tuple{Any}" href="#TensorCast.@reduce-Tuple{Any}"><code>TensorCast.@reduce</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia">@reduce A[i] := sum(j,k) B[i,j,k]             # A = vec(sum(B, dims=(2,3)))
@reduce A[i] := prod(j) B[i] + ε * C[i,j]     # A = vec(prod(B .+ ε .* C, dims=2))
@reduce A[i] = sum(j) exp( C[i,j] / D[j] )    # sum!(A, exp.(C ./ D&#39;) )</code></pre><p>Tensor reduction macro:</p><ul><li>The reduction function can be anything which works like <code>sum(B, dims=(1,3))</code>, for instance <code>prod</code> and <code>maximum</code> and <code>Statistics.mean</code>.</li><li>In-place operations <code>Z[j] = sum(...</code> will construct the banged version of the given function&#39;s name, which must work like <code>sum!(Z, A)</code>.</li><li>The tensors can be anything that <code>@cast</code> understands, including gluing of slices <code>B[i,k][j]</code> and reshaping <code>B[i⊗j,k]</code>. See <code>? @cast</code> for the complete list.</li><li>If there is a function of one or more tensors on the right, then this is a broadcasting operation.</li><li>Index ranges may be given afterwards (as for <code>@cast</code>) or inside the reduction <code>sum(i:3, k:4)</code>.</li><li>All indices appearing on the right must appear either within <code>sum(...)</code> etc, or on the left.</li></ul><pre><code class="language-none">F = @reduce sum(i,j)  B[i] + γ * D[j]         # sum(B .+ γ .* D&#39;)
@reduce G[] := sum(i,j)  B[i] + γ * D[j]      # F == G[]</code></pre><p>Complete reduction to a scalar output <code>F</code>, or a zero-dim array <code>G</code>. <code>G[]</code> involves <code>sum(A, dims=(1,2))</code> rather than <code>sum(A)</code>.</p><pre><code class="language-none">@reduce @lazy Z[k] := sum(i,j) A[i] * B[j] * C[k]  (i in 1:N, j in 1:N, k in 1:N)</code></pre><p>The option <code>@lazy</code> replaces the broadcast expression with a <code>BroadcastArray</code>, to avoid <code>materialize</code>ing the entire array before summing. In the example this is of size <code>N^3</code>. This needs <code>using LazyArrays</code> to work.</p><p>The options <code>@strided</code> and <code>@avx</code> will alter broadcasting operations,  and need <code>using Strided</code> or <code>using LoopVectorization</code> to work.</p><pre><code class="language-none">@reduce sum(i) A[i] * log(@reduce _[i] := sum(j) A[j] * exp(B[i,j]))
@cast W[i] := A[i] * exp(- @reduce S[i] = sum(j) exp(B[i,j]) lazy)</code></pre><p>Recursion like this is allowed, inside either <code>@cast</code> or <code>@reduce</code>. The intermediate array need not have a name, like <code>_[i]</code>, unless writing into an existing array, like <code>S[i]</code> here.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mcabbott/TensorCast.jl/blob/f29d3902927a900fdf692a2dadcf1f98f2948d68/src/macro.jl#L79-L118">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="TensorCast.@matmul-Tuple{Any}" href="#TensorCast.@matmul-Tuple{Any}"><code>TensorCast.@matmul</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia">@matmul M[a,c] := sum(b)  A[a,b] * B[b,c]</code></pre><p>Matrix multiplication macro. Uses the same syntax as <code>@reduce</code>, but instead of broadcasting the expression on the right out to a 3-tensor before summing it along one dimension, this calls <code>*</code> or <code>mul!</code>, which is usually much faster. But it only works on expressions of suitable form.</p><p>With more than two tensors on the right, it proceeds left to right, and each summed index must appear on two tensors, probably neighbours.</p><p>Note that unlike <code>@einsum</code> and <code>@tensor</code>, you must explicitly specify what indices to sum over. Both this macro and <code>@reduce</code> could in principal infer this, and perhaps I will add that later... but then I find myself commenting <code># sum_μ</code> anyway.</p><pre><code class="language-none">@matmul Z[a⊗b,z] = sum(i,j,k)  D[i,a][j] * E[i⊗j,_,k,b] * F[z,3,k]</code></pre><p>Each tensor will be pre-processed exactly as for <code>@cast</code> / <code>@reduce</code>, here glueing slices of <code>D</code> together, reshaping <code>E</code>, and taking a view of <code>F</code>. Once this is done, the right hand side must be of the form <code>(tensor) * (tensor) * ...</code>, which becomes <code>mul!(ZZ, (DD * EE), FF)</code>.</p><pre><code class="language-none">@reduce V[i] := sum(k) W[k] * exp(@matmul _[i,k] := sum(j) A[i,j] * B[j,k])</code></pre><p>You should be able to use this within the other macros, as shown.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mcabbott/TensorCast.jl/blob/f29d3902927a900fdf692a2dadcf1f98f2948d68/src/macro.jl#L125-L150">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="TensorCast.@pretty-Tuple{Any}" href="#TensorCast.@pretty-Tuple{Any}"><code>TensorCast.@pretty</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia">@pretty @cast A[...] := B[...]</code></pre><p>Prints an approximately equivalent expression with the macro expanded. Compared to <code>@macroexpand1</code>, generated symbols are replaced with animal names (from MacroTools), comments are deleted, module names are removed from functions, and the final expression is fed to <code>println()</code>.</p><p>To copy and run the printed expression, you may need various functions which aren&#39;t exported. Try something like <code>using TensorCast: orient, star, rview, @assert_, red_glue, sliceview</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mcabbott/TensorCast.jl/blob/f29d3902927a900fdf692a2dadcf1f98f2948d68/src/pretty.jl#L2-L12">source</a></section></article><h1 id="Functions"><a class="docs-heading-anchor" href="#Functions">Functions</a><a id="Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Functions" title="Permalink"></a></h1><p>These are not exported, but are called by the macros above,  and visible in what <code>@pretty</code> prints out. </p><article class="docstring"><header><a class="docstring-binding" id="TensorCast.Fast.diagview" href="#TensorCast.Fast.diagview"><code>TensorCast.Fast.diagview</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">diagview(M) = view(M, diagind(M))</code></pre><p>Like <code>diag(M)</code> but makes a view.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mcabbott/TensorCast.jl/blob/f29d3902927a900fdf692a2dadcf1f98f2948d68/src/view.jl#L2-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="TensorCast.Fast.sliceview" href="#TensorCast.Fast.sliceview"><code>TensorCast.Fast.sliceview</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">sliceview(A, code)
slicecopy(A, code)</code></pre><p>Slice array <code>A</code> according to <code>code</code>, a tuple of length <code>ndims(A)</code>, in which <code>:</code> indicates a dimension of the slices, and <code>*</code> a dimension separating them. For example if <code>code = (:,*,:)</code> then slices are either <code>view(A, :,i,:)</code> or <code>A[:,i,:]</code> with <code>i=1:size(A,2)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mcabbott/TensorCast.jl/blob/f29d3902927a900fdf692a2dadcf1f98f2948d68/src/slice.jl#L2-L10">source</a></section></article><p>These are from helper packages:</p><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>TensorCast.stack</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>TensorCast.TransmutedDimsArray</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>TensorCast.transmute</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>TensorCast.transmutedims</code>. Check Documenter&#39;s build log for details.</p></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../options/">« Options</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 22 March 2021 05:13">Monday 22 March 2021</span>. Using Julia version 1.5.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
