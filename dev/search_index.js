var documenterSearchIndex = {"docs":
[{"location":"reduce/#Reductions","page":"Reduction","title":"Reductions","text":"","category":"section"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"julia> using TensorCast\n\njulia> M = collect(reshape(1:12, 3,4));","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"This is the basic syntax to sum over one index:","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"julia> @reduce S[i] := sum(j) M[i,j] + 1000\n3-element Array{Int64,1}:\n 4022\n 4026\n 4030\n\njulia> @pretty @reduce S[i] := sum(j) M[i,j] + 1000\nbegin\n    ndims(M) == 2 || throw(ArgumentError(\"expected a 2-tensor M[i, j]\"))\n    S = dropdims(sum(@__dot__(M + 1000), dims = 2), dims = 2)\nend","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"Note that:","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"You must always specify the index to be summed over. \nThe sum applies to the whole right side (including the 1000 here). \nAnd the summed dimensions are always dropped (unless you explicitly say S[i,_] := ...).","category":"page"},{"location":"reduce/#Not-just-sum","page":"Reduction","title":"Not just sum","text":"","category":"section"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"You may use any reduction funciton which understands keyword dims=..., like sum does.  For example:","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"julia> using Statistics\n\njulia> @reduce A[j] := mean(i) M[i,j]^2\n4-element Array{Float64,1}:\n   4.666666666666667\n  25.666666666666668\n  64.66666666666667\n 121.66666666666667","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"If writing into an existing array, then the function must work like sum! does:","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"julia> @pretty @reduce A[j] = mean(i) M[i,j]^2\nbegin\n    local pelican = transmute(M, (2, 1))  # pelican = transpose(M)\n    mean!(A, @__dot__(pelican ^ 2))\nend","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"Otherwise all the same notation as @cast works.  Here's a max-pooling trick with combined indices, in which we group the numbers in R into tiplets and keep the maximum of each set – equivalent to the maximum of each column below.  ","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"julia> R = collect(0:5:149);\n\njulia> @reduce Rmax[_,c] := maximum(r) R[(r,c)]  r in 1:3\n1×10 Array{Int64,2}:\n 10  25  40  55  70  85  100  115  130  145\n\njulia> reshape(R, 3,:)\n3×10 Array{Int64,2}:\n  0  15  30  45  60  75   90  105  120  135\n  5  20  35  50  65  80   95  110  125  140\n 10  25  40  55  70  85  100  115  130  145","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"Here r in 1:3 indicates the range of r, implying c ∈ 1:10. You may also write maximum(r:3) R[(r,c)].","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"In the same way, this down-samples an image by a factor of 2,  taking the mean of each 2×2 block of pixels, in each colour c:","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"julia> @reduce smaller[I,J,c] := mean(i:2, j:2) orig[(i,I), (j,J), c]","category":"page"},{"location":"reduce/#Scalar-output","page":"Reduction","title":"Scalar output","text":"","category":"section"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"Here are different ways to write complete reduction:","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"julia> @reduce Z := sum(i,j) M[i,j]      # Z = sum(M)\n78\n\njulia> Z = @reduce sum(i,j) M[i,j]\n78\n\njulia> @reduce Z0[] := sum(i,j) M[i,j]   # Z0 = dropdims(sum(M, dims=(1, 2)), dims=(1, 2))\n0-dimensional Array{Int64,0}:\n78\n\njulia> @reduce Z1[_] := sum(i,j) M[i,j]\n1-element Array{Int64,1}:\n 78","category":"page"},{"location":"reduce/#Recursion","page":"Reduction","title":"Recursion","text":"","category":"section"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"If you want to sum only part of an expression, or have one sum inside another,  then you can place a complete @reduce expression inside @cast or @reduce. There is no need to name the intermediate array, here termite[x], but you must show its indices:","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"julia> @pretty @reduce sum(x,θ) L[x,θ] * p[θ] * log(L[x,θ] / @reduce _[x] := sum(θ′) L[x,θ′] * p[θ′])\nbegin\n    ndims(L) == 2 || error()  # etc, some checks\n    local goshawk = transmute(p, (nothing, 1))\n    sandpiper = dropdims(sum(@__dot__(L * goshawk), dims = 2), dims = 2)  # inner sum\n    bison = sum(@__dot__(L * goshawk * log(L / sandpiper)))\nend","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"Notice that the inner sum here is a matrix-vector product, so it will be more efficient to  write @matmul [x] := sum(θ') L[x,θ′] * p[θ′] to call L * p instead of broadcasting. ","category":"page"},{"location":"reduce/#Matrix-multiplication","page":"Reduction","title":"Matrix multiplication","text":"","category":"section"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"It's possible to multiply matrices using @reduce,  but this is exceptionally inefficient, as it first broadcasts out a 3-tensor  before summing over one index:","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"julia> @pretty @reduce R[i,k] := sum(j) M[i,j] * N[j,k]\nbegin\n    size(M, 2) == size(N, 1) || error()  # etc, some checks\n    local fish = transmute(N, (nothing, 1, 2))   # fish = reshape(N, 1, size(N)...)\n    R = dropdims(sum(@__dot__(M * fish), dims = 2), dims = 2)\nend","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"The macro @matmul has the same syntax, but instead calls A*B. ","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"julia> A = rand(100,100); B = rand(100,100);\n\njulia> red(A,B) = @reduce R[i,k] := sum(j) A[i,j] * B[j,k];\n\njulia> mul(A,B) = @matmul P[i,k] := sum(j) A[i,j] * B[j,k];\n\njulia> using BenchmarkTools\n\njulia> @btime red($A, $B);\n  1.471 ms (25 allocations: 7.71 MiB)\n\njulia> @btime mul($A, $B);\n  33.489 μs (2 allocations: 78.20 KiB)","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"Of course you may just write A * B yourself, but in general @matmul will handle the same  reshaping, transposing, fixed indices, etc. steps as @reduce does.  Once all of that is done, however, the result must be a product like A * B in which the indices being summed over appear on both tensors. ","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"To use the more flexible @reduce in cases like this,  when creating the large intermediate tensor will be expensive, the option @lazy which creates a LazyArrays.BroadcastArray instead can help: ","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"julia> using LazyArrays\n\njulia> lazyred(A,B) = @reduce @lazy R[i,k] := sum(j) A[i,j] * B[j,k];\n\njulia> @btime lazyred($A, $B);\n  223.172 μs (26 allocations: 78.95 KiB)  # when things worked well\n  10.186 ms (23 allocations: 78.77 KiB)   # today\n\njulia> red(A, B) ≈ mul(A, B) ≈ lazyred(A,B)\ntrue","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"More details on the next page. ","category":"page"},{"location":"reduce/#No-reduction","page":"Reduction","title":"No reduction","text":"","category":"section"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"The syntax of @reduce can also be used with @cast, to apply functions which  take a dims keyword, but do not produce trivial dimensions. ","category":"page"},{"location":"reduce/","page":"Reduction","title":"Reduction","text":"julia> normalise(p; dims) = p ./ sum(p; dims=dims);\n\njulia> @cast N[x,y] := normalise(x) M[x,y]\n3×4 Array{Float64,2}:\n 0.166667  0.266667  0.291667  0.30303\n 0.333333  0.333333  0.333333  0.333333\n 0.5       0.4       0.375     0.363636","category":"page"},{"location":"basics/#Basics","page":"Basics","title":"Basics","text":"","category":"section"},{"location":"basics/","page":"Basics","title":"Basics","text":"Install and set up:","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"(v1.5) pkg> add TensorCast","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> using TensorCast\n\njulia> V = [10,20,30];\n\njulia> M = collect(reshape(1:12, 3,4))\n3×4 Array{Int64,2}:\n 1  4  7  10\n 2  5  8  11\n 3  6  9  12","category":"page"},{"location":"basics/#Broadcasting","page":"Basics","title":"Broadcasting","text":"","category":"section"},{"location":"basics/","page":"Basics","title":"Basics","text":"Here's a simple use of @cast, broadcasting addition over a matrix, a vector, and a scalar.  Using := makes a new array, after which the left and right will be equal for all possible  values of the indices:","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> @cast A[j,i] := M[i,j] + 10 * V[i]\n4×3 Array{Int64,2}:\n 101  202  303\n 104  205  306\n 107  208  309\n 110  211  312\n\njulia> all(A[j,i] == M[i,j] + 10 * V[i] for i in 1:3, j in 1:4)\ntrue","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"To make this happen, first M is transposed, and then the axis of V is re-oriented  to lie in the second dimension. The macro @pretty prints out what @cast produces:","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> @pretty @cast A[j,i] := M[i,j] + 10 * V[i]\nbegin\n    local panda = transmute(M, (2, 1))\n    local bat = transmute(V, (nothing, 1))\n    A = @__dot__(panda + 10bat)\nend","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"The function transmute is a generalised version of permutedims. It transposes M, and then places V's first dimension to lie along the second dimension – also a transpose, here, but it allows for things like transmute(M, (2,nothing,1)). This is from TransmuteDims.jl.","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"Of course @__dot__ is the full name of the broadcasting macro @.,  which simply produces A = pelican .+ 10 .* termite. And the animal names (in place of generated symbols) are from the indispensible MacroTools.jl.","category":"page"},{"location":"basics/#Slicing","page":"Basics","title":"Slicing","text":"","category":"section"},{"location":"basics/","page":"Basics","title":"Basics","text":"Here are two ways to slice M into its columns, both of these produce S = sliceview(M, (:, *)) which is simply collect(eachcol(M)):","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> @cast S[j][i] := M[i,j]\n4-element Array{SubArray{Int64,1,Array{Int64,2},Tuple{Base.Slice{Base.OneTo{Int64}},Int64},true},1}:\n [1, 2, 3]\n [4, 5, 6]   \n [7, 8, 9]   \n [10, 11, 12]\n\njulia> all(S[j][i] == M[i,j] for i in 1:3, j in 1:4)\ntrue\n\njulia> @cast S2[j] := M[:,j]; \n\njulia> S == S2\ntrue","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"We can glue slices back together with the same notation, M2[i,j] := S[j][i].  Combining this with slicing from M[:,j] is a convenient way to perform mapslices operations:","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> @cast C[i,j] := cumsum(M[:,j])[i]\n 3×4 stack(::Array{Array{Int64,1},1}) with eltype Int64:\n 1   4   7  10\n 3   9  15  21\n 6  15  24  33\n\njulia> C == mapslices(cumsum, M, dims=1)\ntrue","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"This is both faster and more general than mapslices,  as it does not have to infer what shape the function produces:","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> using BenchmarkTools\n\njulia> f(M) = @cast _[i,j] := cumsum(M[:,j])[i];\n\njulia> M10 = rand(10,1000);\n\njulia> @btime mapslices(cumsum, $M10, dims=1);\n  630.725 μs (7508 allocations: 400.28 KiB)\n\njulia> @btime f($M10);\n  64.056 μs (2006 allocations: 297.25 KiB)","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"It's also more readable, in less trivial examples:","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> using LinearAlgebra, Random; Random.seed!(42);\n\njulia> T = rand(3,3,5);\n\njulia> @cast E[i,n] := eigen(T[:,:,n]).values[i];\n\njulia> E == dropdims(mapslices(x -> eigen(x).values, T, dims=(1,2)), dims=2)\ntrue","category":"page"},{"location":"basics/#Fixed-indices","page":"Basics","title":"Fixed indices","text":"","category":"section"},{"location":"basics/","page":"Basics","title":"Basics","text":"Sometimes it's useful to insert a trivial index into the output – for instance  to match the output of mapslices(x -> eigen(... just above:","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> @cast E3[i,_,n] := eigen(T[:,:,n]).values[i];\n\njulia> size(E3)\n(3, 1, 5)","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"Using _ on the right will demand that size(E3,2) == 1; but you can also fix indices to other values.  If these are variables not integers, they must be interpolated M[$row,j]  to distinguish them from index names:","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> col = 3;\n\njulia> @cast O[i,j] := (M[i,1], M[j,$col])\n3×3 Array{Tuple{Int64,Int64},2}:\n (1, 7)  (1, 8)  (1, 9)\n (2, 7)  (2, 8)  (2, 9)\n (3, 7)  (3, 8)  (3, 9)","category":"page"},{"location":"basics/#Reshaping","page":"Basics","title":"Reshaping","text":"","category":"section"},{"location":"basics/","page":"Basics","title":"Basics","text":"Sometimes it's useful to combine two (or more) indices into one,  which may be written  either (i,j) or i⊗j:","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> vec(M) == @cast _[(i,j)] := M[i,j]\ntrue","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"The next-simplest version of this is precisely what the built-in function kron does:","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> W = 1 ./ [10,20,30,40];\n\njulia> @cast K[_, i⊗j] := V[i] * W[j]\n1×12 Array{Float64,2}:\n 1.0  2.0  3.0  0.5  1.0  1.5  0.333333  0.666667  1.0  0.25  0.5  0.75\n\njulia> @cast K[1, (i,j)] := V[i] * W[j]  # identical!\n1×12 Array{Float64,2}:\n 1.0  2.0  3.0  0.5  1.0  1.5  0.333333  0.666667  1.0  0.25  0.5  0.75\n\njulia> K == kron(W, V)'\ntrue\n\njulia> all(K[i + 3(j-1)] == V[i] * W[j] for i in 1:3, j in 1:4)\ntrue\n\njulia> Base.kron(A::Array{T,3}, X::Array{T′,3}) where {T,T′} =    # extend kron to 3-tensors\n           @cast _[x⊗a, y⊗b, z⊗c] := A[a,b,c] * X[x,y,z]","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"If an array on the right has a combined index, then it may be ambiguous how to divide up its range. You can resolve this by providing explicit ranges, after the main expression: ","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> @cast A[i,j] := collect(1:12)[i⊗j]  i in 1:2\n2×6 Array{Int64,2}:\n 1  3  5  7   9  11\n 2  4  6  8  10  12\n\njulia> @cast A[i,j] := collect(1:12)[i⊗j]  (i ∈ 1:4, j ∈ 1:3)\n4×3 Array{Int64,2}:\n 1  5   9\n 2  6  10\n 3  7  11\n 4  8  12","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"Writing into a given array with = instead of := will also remove ambiguities,  as size(A) is known:","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> @cast A[i,j] = 10 * collect(1:12)[i⊗j];","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"If the right hand side is independent of an index, then the same result is repeated.  The range of the index must still be known:","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> @cast R[r,(n,c)] := M[r,c]^2  (n in 1:3)\n3×12 Array{Int64,2}:\n 1  1  1  16  16  16  49  49  49  100  100  100\n 4  4  4  25  25  25  64  64  64  121  121  121\n 9  9  9  36  36  36  81  81  81  144  144  144\n\njulia> R == repeat(M .^ 2, inner=(1,3))\ntrue\n\njulia> @cast R[r,(c,n)] = M[r,c]  # repeat(M, outer=(1,3)), uses size(R)\n3×12 Array{Int64,2}:\n 1  4  7  10  1  4  7  10  1  4  7  10\n 2  5  8  11  2  5  8  11  2  5  8  11\n 3  6  9  12  3  6  9  12  3  6  9  12","category":"page"},{"location":"basics/#Glue-and-reshape","page":"Basics","title":"Glue & reshape","text":"","category":"section"},{"location":"basics/","page":"Basics","title":"Basics","text":"As a less trivial example of these combined indices, here is one way to combine a list of matrices into one large grid. Notice that x varies faster than i, and so on – the linear order of index x⊗i agrees with that of two indices x,i. ","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> list = [fill(k,2,2) for k in 1:8];\n\njulia> @cast mat[x⊗i, y⊗j] |= list[i⊗j][x,y]  i in 1:2\n4×8 Array{Int64,2}:\n 1  1  3  3  5  5  7  7\n 1  1  3  3  5  5  7  7\n 2  2  4  4  6  6  8  8\n 2  2  4  4  6  6  8  8\n\njulia> vec(mat) == @cast _[xi⊗yj] := mat[xi, yj]\ntrue\n\njulia> mat == hvcat((4,4), transpose(reshape(list,2,4))...)\ntrue","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"Alternatively, this reshapes each matrix to a vector, and makes them columns of the output:","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> @cast colwise[x⊗y,i] := (list[i][x,y])^2\n4×8 Array{Int64,2}:\n 1  4  9  16  25  36  49  64\n 1  4  9  16  25  36  49  64\n 1  4  9  16  25  36  49  64\n 1  4  9  16  25  36  49  64\n\njulia> colwise == reduce(hcat, vec.(list)) .^ 2\ntrue","category":"page"},{"location":"basics/#Index-values","page":"Basics","title":"Index values","text":"","category":"section"},{"location":"basics/","page":"Basics","title":"Basics","text":"Mostly the indices appearing in @cast expressions are just notation, to indicate what permutation / reshape is required.  But if an index appears outside of square brackets, this is understood as a value, implemented by broadcasting over a range (appropriately permuted):","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> @cast _[i,j] := M[i,j]^2 * (i >= j)\n3×4 Array{Int64,2}:\n 1   0   0  0\n 4  25   0  0\n 9  36  81  0\n\njulia> ans == M .^2 .* (axes(M,1) .>= transpose(axes(M,2)))  # what this generates\ntrue\n\njulia> using OffsetArrays\n\njulia> @cast _[r,c] := r^2 + c^2  (r in -1:1, c in -7:7)\n3×15 OffsetArray(::Array{Int64,2}, -1:1, -7:7) with eltype Int64 with indices -1:1×-7:7:\n 50  37  26  17  10  5  2  1  2  5  10  17  26  37  50\n 49  36  25  16   9  4  1  0  1  4   9  16  25  36  49\n 50  37  26  17  10  5  2  1  2  5  10  17  26  37  50","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"Writing $i will interpolate the variable i, distinct from the index i:","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> i, k = 10, 100;\n\njulia> @cast ones(3)[i] = i + $i + k\n3-element Vector{Float64}:\n 111.0\n 112.0\n 113.0","category":"page"},{"location":"basics/#Reverse-and-shuffle","page":"Basics","title":"Reverse & shuffle","text":"","category":"section"},{"location":"basics/","page":"Basics","title":"Basics","text":"A minus in front of an index will reverse that direction, and a tilde will shuffle it.  Both create views, which you may explicitly collect using |=:","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> @cast M2[i,j] := M[i,-j]\n3×4 view(::Array{Int64,2}, :, 4:-1:1) with eltype Int64:\n 10  7  4  1\n 11  8  5  2\n 12  9  6  3\n\njulia> all(M2[i,j] == M[i, end+begin-j] for i in 1:3, j in 1:4)\ntrue\n\njulia> using Random; Random.seed!(42); \n\njulia> @cast M3[i,j] |= M[i,~j]\n3×4 Array{Int64,2}:\n 7  4  1  10\n 8  5  2  11\n 9  6  3  12","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"Note that the minus is a slight deviation from the rule that left equals right for all indices, it should really be M[i, end+1-j].","category":"page"},{"location":"basics/#Primes-'","page":"Basics","title":"Primes '","text":"","category":"section"},{"location":"basics/","page":"Basics","title":"Basics","text":"Acting on indices, A[i'] is normalised to A[i′] unicode \\prime (which looks identical in some fonts). Acting on elements, C[i,j]' means adjoint.(C), elementwise complex conjugate, equivalent to (C')[j,i]. If the elements are matrices, as in C[:,:,k]', then  adjoint is conjugate transpose.","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> @cast C[i,i'] := (1:4)[i⊗i′] + im  (i ∈ 1:2, i′ ∈ 1:2)\n2×2 Array{Complex{Int64},2}:\n 1+1im  3+1im\n 2+1im  4+1im\n\njulia> @cast _[i,j] := C[i,j]'\n2×2 Array{Complex{Int64},2}:\n 1-1im  3-1im\n 2-1im  4-1im\n\njulia> C' == @cast _[j,i] := C[i,j]'\ntrue\n\njulia> @cast cubes[1,k′] := (k')^3  (k' in 1:5)  # k' outside square brackets\n1×5 Array{Int64,2}:\n 1  8  27  64  125","category":"page"},{"location":"basics/#Splats","page":"Basics","title":"Splats","text":"","category":"section"},{"location":"basics/","page":"Basics","title":"Basics","text":"You can use a slice of an array as the arguments of a function, or a struct:","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> struct Tri{T} x::T; y::T; z::T end;\n\njulia> @cast triples[k] := Tri(M[:,k]...)\n4-element Array{Tri{Int64},1}:\n Tri{Int64}(1, 2, 3)\n Tri{Int64}(4, 5, 6)\n Tri{Int64}(7, 8, 9)\n Tri{Int64}(10, 11, 12)\n\njulia> ans == Base.splat(Tri).(eachcol(M))\ntrue","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"This one can also be done as reinterpret(reshape, Tri{Int64}, M).","category":"page"},{"location":"basics/#Arrays-of-functions","page":"Basics","title":"Arrays of functions","text":"","category":"section"},{"location":"basics/","page":"Basics","title":"Basics","text":"Besides arrays of numbers (and arrays of arrays) you can also broadcast an array of functions, which is done by calling Core._apply(f, xs...) = f(xs...): ","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> funs = [identity, sqrt];\n\njulia> @cast applied[i,j] := funs[i](V[j])\n2×3 Array{Real,2}:\n 10        20        30      \n  3.16228   4.47214   5.47723\n\njulia> @pretty @cast applied[i,j] := funs[i](V[j])\nbegin\n    local pelican = transmute(V, (nothing, 1))\n    applied = @__dot__(_apply(funs, pelican))\nend","category":"page"},{"location":"basics/#Repeated-indices","page":"Basics","title":"Repeated indices","text":"","category":"section"},{"location":"basics/","page":"Basics","title":"Basics","text":"The only situation in which repeated indices are allowed is when they either  extract the diagonal of a matrix, or create a diagonal matrix:","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"julia> @cast D[i] |= C[i,i]\n2-element Array{Complex{Int64},1}:\n 1 + 1im\n 4 + 1im\n\njulia> D2 = @cast _[i,i] := V[i]\n3×3 Diagonal{Int64,Array{Int64,1}}:\n 10   ⋅   ⋅\n  ⋅  20   ⋅\n  ⋅   ⋅  30","category":"page"},{"location":"basics/","page":"Basics","title":"Basics","text":"All indices appearing on the right must also appear on the left.  There is no implicit sum over repeated indices on different tensors. To sum over things, you need @reduce or @matmul, described on the next page.","category":"page"},{"location":"options/#Options","page":"Options","title":"Options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"Expressions with = write into an existing array,  while those with := do not. This is the same notation as  TensorOperations.jl and Einsum.jl.  But unlike those packages, sometimes the result of @cast is a view of the original, for instance  @cast A[i,j] := B[j,i] gives A = transpose(B). You can forbid this, and insist on a copy,  by writing |= instead (or passing the option collect). ","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"Various other options can be given after the main expression. ","category":"page"},{"location":"options/#Ways-of-slicing","page":"Options","title":"Ways of slicing","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"The default way of slicing creates an array of views,  but if you use |=, or the option lazy=false, then instead then you get copies: ","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"M = rand(1:99, 3,4)\n\n@cast S[k][i] := M[i,k]             # collect(eachcol(M)) ≈ [view(M,:,k) for k in 1:4]\n@cast S[k][i] |= M[i,k]             # [M[:,k] for k in 1:4]\n@cast S[k][i] := M[i,k] lazy=false  # the same","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"The default way of un-slicing is reduce(hcat, ...), which creates a new array.  But there are other options, controlled by keywords after the expression:","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"@cast A[i,k] := S[k][i] lazy=false  # A = reduce(hcat, B)\n@cast A[i,k] := S[k][i]             # A = LazyStack.stack(B)\n\nsize(A) == (3, 4) # true","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"Another kind of slices are provided by StaticArrays.jl, in which a Vector of SVectors is just a different interpretation of the same memory as a Matrix.  By another slight abuse of notation, such slices are written here as curly brackets:","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"@cast S[k]{i} := M[i,k]  i in 1:3   # S = reinterpret(SVector{3,Int}, vec(M)) \n@cast S[k] := M{:3, k}              # equivalent\n\n@cast R[k,i] := S[k]{i}             # such slices can be reinterpreted back again","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"Both S and R here are views of the original matrix M.  When creating such slices, their size ought to be provided, either as a literal integer or  through the types. Note that you may also write S[k]{i:3} = .... ","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"The second notation (with M{:,k}) is useful for mapslices. Continuing the example: ","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"M10 = rand(10,1000); \nmapslices(cumsum, M10, dims=1)          # 630 μs using @btime\n@cast [i,j] := cumsum(M10[:,j])[i]      #  64 μs\n@cast [i,j] := cumsum(M10{:10,j})[i]    #  38 μs","category":"page"},{"location":"options/#Better-broadcasting","page":"Options","title":"Better broadcasting","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"The package Strided.jl can apply multi-threading to  broadcasting, and some other magic. You can enable it like this: ","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"using Strided  # and export JULIA_NUM_THREADS = 4 before starting\nA = randn(4000,4000); B = similar(A);\n\n@time @cast B[i,j] = (A[i,j] + A[j,i])/2;             # 0.12 seconds\n@time @cast @strided B[i,j] = (A[i,j] + A[j,i])/2;    # 0.025 seconds","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"The package LoopVectorization.jl provides  a macro @avx which modifies broadcasting to use vectorised instructions.  This can be used as follows:","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"using LoopVectorization, BenchmarkTools\nC = randn(40,40); \n\nD = @btime @cast [i,j] := exp($C[i,j]);         # 13 μs\nD′ = @btime @cast @avx [i,j] := exp($C[i,j]);   #  3 μs\nD ≈ D′","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"When broadcasting and then summing over some directions, it can be faster to avoid creating the  entire array, then throwing it away. This can be done with the package  LazyArrays.jl which has a lazy BroadcastArray.  In the following example, the product V .* V' .* V3 contains about 1GB of data,  the writing of which is avoided by giving the option lazy: ","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"using LazyArrays\nV = rand(500); V3 = reshape(V,1,1,:);\n\n@time @reduce W[i] := sum(j,k) V[i]*V[j]*V[k];        # 0.6 seconds, 950 MB\n@time @reduce @lazy W[i] := sum(j,k) V[i]*V[j]*V[k];  # 0.025 s, 5 KB","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"However, right now this gives 2.1 seconds (16 allocations: 4.656 KiB),  so it seems to be saving memory but not time. Something is broken!","category":"page"},{"location":"options/#Less-lazy","page":"Options","title":"Less lazy","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"To disable the default use of PermutedDimsArray-like wrappers, etc, give the option lazy=false: ","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"@pretty @cast Z[y,x] := M[x,-y]  lazy=false\n# Z = transmutedims(reverse(M, dims = 2), (2, 1))\n\n@pretty @cast Z[y,x] := M[x,-y] \n# Z = transmute(Reverse{2}(M), (2, 1))  # transpose(@view M[:, end:-1:begin])","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"This also controls how the extraction of diagonal elements and creation of diagonal matrices are done:","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"@pretty @cast M[i,i] := A[i,i]  lazy=false\n# M = diagm(0 => diag(A))\n\n@pretty @cast D[i,i] := A[i,i]\n# D = Diagonal(diagview(A))  # Diagonal(view(A, diagind(A)))\n\n@pretty @cast M[i,i] = A[i,i]  lazy=false  # into diagonal of existing matrix M\n# diagview(M) .= diag(A); M","category":"page"},{"location":"options/#Gradients","page":"Options","title":"Gradients","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"I moved some code here from SliceMap.jl, which I should describe!","category":"page"},{"location":"#[TensorCast.jl](https://github.com/mcabbott/TensorCast.jl)","page":"Home","title":"TensorCast.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package lets you write complicated formulae in index notation, which are turned into Julia's usual broadcasting, permuting, slicing, and reducing operations. It does little you couldn't do yourself, but provides a notation in which it is often  easier to confirm that you are doing what you intend.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Source, issues, etc: github.com/mcabbott/TensorCast.jl","category":"page"},{"location":"#Changes","page":"Home","title":"Changes","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Version 0.2 was a re-write, see the release notes to know what changed.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Version 0.4 has significant changes:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Broadcasting options and index ranges are now written @cast @avx A[i,j] := B[i⊗j] (i ∈ 1:3) instead of @cast A[i,j] := B[i⊗j] i:3, axv (using LoopVectorization.jl for the broadcast, and supplying the range of i).\nTo return an array without naming it, write an underscore @cast _[i] := ... rather than omitting it entirely.\nSome fairly obscure features have been removed for simplicity: Indexing by an array @cast A[i,k] := B[i,J[k]] and by a range @cast C[i] := f(D[1:3, i]) will no longer work.\nSome dimension checks are inserted by default; previously the option assert did this.\nIt uses LazyStack.jl to combine handles slices, simplifying earlier code. This is lazier by default, write @cast A[i,k] := log(B[k][i]) lazy=false (with a new keyword option) to glue into an Array before broadcasting.\nIt uses TransmuteDims.jl to handle all permutations & many reshapes. This is lazier by default – the earlier code sometimes copied to avoid reshaping a PermutedDimsArray. This isn't always faster, though, and can be disabled by lazy=false.","category":"page"},{"location":"","page":"Home","title":"Home","text":"New features in 0.4:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Indices can appear ouside of indexing: @cast A[i,j] = i+j translates to A .= axes(A,1) .+ axes(A,2)'\nThe ternary operator ? : can appear on the right, and will be broadcast correctly.\nAll operations should now support OffsetArrays.jl.","category":"page"},{"location":"#Pages","page":"Home","title":"Pages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Use of @cast for broadcasting, dealing with arrays of arrays, and generalising mapslices\n@reduce and @matmul, for taking the sum (or the maximum, etc) over some dimensions\nOptions: broadcasting with Strided.jl, LoopVectorization.jl, LazyArrays.jl, and slicing with StaticArrays.jl\nDocstrings, which list the complete set of possibilities.","category":"page"},{"location":"docstrings/#Macros","page":"Docstrings","title":"Macros","text":"","category":"section"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"CurrentModule = TensorCast","category":"page"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"@cast(ex)","category":"page"},{"location":"docstrings/#TensorCast.@cast-Tuple{Any}","page":"Docstrings","title":"TensorCast.@cast","text":"@cast Z[i,j,...] := f(A[i,j,...], B[j,k,...])  options\n\nMacro for broadcasting, reshaping, and slicing of arrays in index notation. Understands the following things:\n\nA[i,j,k] is a three-tensor with these indices.\nB[(i,j),k] is the same thing, reshaped to a matrix. Its first axis (the bracket) is indexed by n = i + (j-1) * N where i ∈ 1:N. This may also be written B[i⊗j,k].\nC[k][i,j] is a vector of matrices, either created by slicing (if on the left) or implying glueing into (if on the right) a 3-tensor A[i,j,k].\nD[j,k]{i} is an ordinary matrix of SVectors, which may be reinterpreted from A[i,j,k].\nE[i,_,k] has two nontrivial dimensions, and size(E,2)==1. On the right hand side (or when writing to an existing array) you may also write E[i,3,k] meaning view(E, :,3,:), or E[i,$c,j] to use a variable c.\nf(x)[i,j,k] is allowed; f(x) must return a 3-tensor (and will be evaluated only once).\ng(H[:,k])[i,j] is a generalised mapslices, with g mapping columns of H   to matrices, which are glued into a 3-tensor A[i,j,k].\nh(I[i], J[j])[k] expects an h which maps two scalars to a vector, which gets broadcasted h.(I,J'), then glued to make a 3-tensor.\nK[i,j]' conjugates each element, equivalent to K'[j,i] which is the conjugate-transpose of the matrix.\nM[i,i] means diag(M)[i], but only for matrices: N[i,i,k] is an error.\nP[i,i'] is normalised to P[i,i′] with unicode \\prime.\nR[i,-j,k] means roughly reverse(R, dims=2), and Q[i,~j,k] similar with shuffle.\n\nThe left and right hand sides must have all the same indices, and the only repeated index allowed is M[i,i], which is a diagonal not a trace. See @reduce and @matmul for related macros which can sum over things.\n\nIf a function of one or more tensors appears on the right hand side, then this represents a broadcasting operation, and the necessary re-orientations of axes are automatically inserted.\n\nThe following actions are possible:\n\n= writes into an existing array, overwriting its contents, while += adds (precisely Z .= Z .+ ...) and *= multiplies.\n:= creates a new array. To omit the name, write Z = @cast _[i,j,k] := ....\n|= insists that the result is an Array not a view, or some other lazy wapper. (This may still be a reshape of the input, it does not guarantee a copy.)\n\nOptions specified at the end (if several, separated by ,) are:\n\ni in 1:3 or i ∈ 1:3 supplies the range of index i. Variables and functions like j in 1:Nj, k in 1:length(K) are allowed, but i = 1:3 is not.\nlazy=false disables PermutedDimsArray in favour of permutedims,  and Diagonal in favour of diagm for Z[i,i] output.\n\nSome modifications to broadcasting are possible, after loading the corresponding package:\n\n@cast @strided Z[i,j] := ... uses Strided.jl's macro, for multi-threaded broadcasting.\n@cast @avx Z[i,j] := ... uses LoopVectorization.jl's macro, for SIMD acceleration.\n@cast @lazy Z[i,j] := ... uses LazyArrays.jl's BroadcastArray type, although there is no such macro.\n\nTo create static slices D[k]{i,j} you should give all slice dimensions explicitly. You may write D[k]{i:2,j:2} to specify Size(2,2) slices. They are made most cleanly from the first indices of the input, i.e. this D from A[i,j,k]. The notation A{:,:,k} will only work in this order, and writing A{:2,:2,k} provides the sizes.\n\n\n\n\n\n","category":"macro"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"@reduce(ex)","category":"page"},{"location":"docstrings/#TensorCast.@reduce-Tuple{Any}","page":"Docstrings","title":"TensorCast.@reduce","text":"@reduce A[i] := sum(j,k) B[i,j,k]             # A = vec(sum(B, dims=(2,3)))\n@reduce A[i] := prod(j) B[i] + ε * C[i,j]     # A = vec(prod(B .+ ε .* C, dims=2))\n@reduce A[i] = sum(j) exp( C[i,j] / D[j] )    # sum!(A, exp.(C ./ D') )\n\nTensor reduction macro:\n\nThe reduction function can be anything which works like sum(B, dims=(1,3)), for instance prod and maximum and Statistics.mean.\nIn-place operations Z[j] = sum(... will construct the banged version of the given function's name, which must work like sum!(Z, A).\nThe tensors can be anything that @cast understands, including gluing of slices B[i,k][j] and reshaping B[i⊗j,k]. See ? @cast for the complete list.\nIf there is a function of one or more tensors on the right, then this is a broadcasting operation.\nIndex ranges may be given afterwards (as for @cast) or inside the reduction sum(i:3, k:4).\nAll indices appearing on the right must appear either within sum(...) etc, or on the left.\n\nF = @reduce sum(i,j)  B[i] + γ * D[j]         # sum(B .+ γ .* D')\n@reduce G[] := sum(i,j)  B[i] + γ * D[j]      # F == G[]\n\nComplete reduction to a scalar output F, or a zero-dim array G. G[] involves sum(A, dims=(1,2)) rather than sum(A).\n\n@reduce @lazy Z[k] := sum(i,j) A[i] * B[j] * C[k]  (i in 1:N, j in 1:N, k in 1:N)\n\nThe option @lazy replaces the broadcast expression with a BroadcastArray, to avoid materializeing the entire array before summing. In the example this is of size N^3. This needs using LazyArrays to work.\n\nThe options @strided and @avx will alter broadcasting operations,  and need using Strided or using LoopVectorization to work.\n\n@reduce sum(i) A[i] * log(@reduce _[i] := sum(j) A[j] * exp(B[i,j]))\n@cast W[i] := A[i] * exp(- @reduce S[i] = sum(j) exp(B[i,j]) lazy)\n\nRecursion like this is allowed, inside either @cast or @reduce. The intermediate array need not have a name, like _[i], unless writing into an existing array, like S[i] here.\n\n\n\n\n\n","category":"macro"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"@matmul(ex)","category":"page"},{"location":"docstrings/#TensorCast.@matmul-Tuple{Any}","page":"Docstrings","title":"TensorCast.@matmul","text":"@matmul M[a,c] := sum(b)  A[a,b] * B[b,c]\n\nMatrix multiplication macro. Uses the same syntax as @reduce, but instead of broadcasting the expression on the right out to a 3-tensor before summing it along one dimension, this calls * or mul!, which is usually much faster. But it only works on expressions of suitable form.\n\nNote that unlike @einsum and @tensor, you must explicitly specify what indices to sum over.\n\n@matmul Z[a⊗b,k] = sum(i,j)  D[i,a][j] * E[i⊗j,_,k,b]\n\nEach tensor will be pre-processed exactly as for @cast / @reduce, here glueing slices of D together, reshaping E and the output Z. Once this is done, the right hand side must be of the form (tensor) * (tensor), which becomes mul!(ZZ, DD, EE).\n\n@reduce V[i] := sum(k) W[k] * exp(@matmul _[i,k] := sum(j) A[i,j] * B[j,k])\n\nYou should be able to use this within the other macros, as shown.\n\n\n\n\n\n","category":"macro"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"@pretty(ex)","category":"page"},{"location":"docstrings/#TensorCast.@pretty-Tuple{Any}","page":"Docstrings","title":"TensorCast.@pretty","text":"@pretty @cast A[...] := B[...]\n\nPrints an approximately equivalent expression with the macro expanded. Compared to @macroexpand1, generated symbols are replaced with animal names (from MacroTools), comments are deleted, module names are removed from functions, and the final expression is fed to println().\n\nTo copy and run the printed expression, you may need various functions which aren't exported. Try something like using TensorCast: orient, star, rview, @assert_, red_glue, sliceview\n\n\n\n\n\n","category":"macro"},{"location":"docstrings/#Functions","page":"Docstrings","title":"Functions","text":"","category":"section"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"These are not exported, but are called by the macros above,  and visible in what @pretty prints out. ","category":"page"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"TensorCast.diagview","category":"page"},{"location":"docstrings/#TensorCast.Fast.diagview","page":"Docstrings","title":"TensorCast.Fast.diagview","text":"diagview(M) = view(M, diagind(M))\n\nLike diag(M) but makes a view.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"TensorCast.sliceview","category":"page"},{"location":"docstrings/#TensorCast.Fast.sliceview","page":"Docstrings","title":"TensorCast.Fast.sliceview","text":"sliceview(A, code)\nslicecopy(A, code)\n\nSlice array A according to code, a tuple of length ndims(A), in which : indicates a dimension of the slices, and * a dimension separating them. For example if code = (:,*,:) then slices are either view(A, :,i,:) or A[:,i,:] with i=1:size(A,2).\n\n\n\n\n\n","category":"function"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"These are from helper packages:","category":"page"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"TensorCast.stack","category":"page"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"TensorCast.TransmutedDimsArray","category":"page"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"TensorCast.transmute","category":"page"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"TensorCast.transmutedims","category":"page"}]
}
