var documenterSearchIndex = {"docs":
[{"location":"reduce/#Reductions-1","page":"Reduction","title":"Reductions","text":"","category":"section"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"julia> using TensorCast\n\njulia> M = collect(reshape(1:12, 3,4));","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"This is the basic syntax to sum over one index:","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"julia> @reduce S[i] := sum(j) M[i,j] + 1000\n3-element Array{Int64,1}:\n 4022\n 4026\n 4030\n\njulia> @pretty @reduce S[i] := sum(j) M[i,j] + 1000\nbegin\n    S = dropdims(sum(@__dot__(M + 1000), dims=2), dims=2)\nend","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"Note that:","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"You must always specify the index to be summed over. \nThe sum applies to the whole right side (including the 1000 here). \nAnd the summed dimensions are always dropped (unless you explicitly say S[i,_] := ...).","category":"page"},{"location":"reduce/#Not-just-sum-1","page":"Reduction","title":"Not just sum","text":"","category":"section"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"You may use any reduction funciton which understands keyword dims=..., like sum does.  For example:","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"julia> using Statistics\n\njulia> @reduce A[j] := mean(i) M[i,j]^2\n4-element Array{Float64,1}:\n   4.666666666666666\n  25.666666666666664\n  64.66666666666666 \n 121.66666666666666 ","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"If writing into an existing array, then the function must work like sum! does:","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"julia> @pretty @reduce A[j] = mean(i) M[i,j]^2\nbegin\n    local pelican = PermuteDims(M)\n    mean!(A, @__dot__(pelican ^ 2))\nend","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"Otherwise all the same notation as @cast works.  Here's a max-pooling trick with combined indices, in which we group the numbers in R into tiplets and keep the maximum of each set – equivalent to the maximum of each column below.  ","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"julia> R = collect(0:5:149);\n\njulia> @reduce Rmax[_,c] := maximum(r) R[r\\c]  r:3\n1×10 LinearAlgebra.Transpose{Int64,Array{Int64,1}}:\n 10  25  40  55  70  85  100  115  130  145\n\njulia> reshape(R, 3,:)\n3×10 Array{Int64,2}:\n  0  15  30  45  60  75   90  105  120  135\n  5  20  35  50  65  80   95  110  125  140\n 10  25  40  55  70  85  100  115  130  145","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"Here r:3 indicates the range of r, implying c ∈ 1:10.  You may also write maximum(r:3) R[r\\c]. ","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"In the same way, this down-samples an image by a factor of 2,  taking the mean of each 2×2 block of pixels, in each colour c:","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"julia> @reduce smaller[I,J,c] := mean(i:2, j:2) orig[i\\I, j\\J, c]","category":"page"},{"location":"reduce/#Scalar-output-1","page":"Reduction","title":"Scalar output","text":"","category":"section"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"Here are different ways to write complete reduction:","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"julia> @reduce Z := sum(i,j) M[i,j]      # Z = sum(M)\n78\n\njulia> Z = @reduce sum(i,j) M[i,j]\n78\n\njulia> @reduce Z0[] := sum(i,j) M[i,j]   # Z0 = dropdims(sum(M, dims=(1, 2)), dims=(1, 2))\n0-dimensional Array{Int64,0}:\n78\n\njulia> @reduce Z1[_] := sum(i,j) M[i,j]\n1-element Array{Int64,1}:\n 78","category":"page"},{"location":"reduce/#Recursion-1","page":"Reduction","title":"Recursion","text":"","category":"section"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"If you want to sum only part of an expression, or have one sum inside another,  then you can place a complete @reduce expression inside @cast or @reduce. There is no need to name the intermediate array, here termite[x], but you must show its indices:","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"julia> @pretty @reduce sum(x,θ) L[x,θ] * p[θ] * log(L[x,θ] / @reduce [x] := sum(θ′) L[x,θ′] * p[θ′])\nbegin\n    local pelican = orient(p, (*, :))\n    termite = dropdims(sum(@__dot__(L * pelican), dims=2), dims=2)\n    local caterpillar = orient(p, (*, :))\n    goat = sum(@__dot__(L * caterpillar * log(L / termite)))\nend","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"Notice that the inner sum here is a matrix-vector product, so it will be more efficient to  write @matmul [x] := sum(θ') L[x,θ′] * p[θ′] to call L * p instead of broadcasting. ","category":"page"},{"location":"reduce/#Matrix-multiplication-1","page":"Reduction","title":"Matrix multiplication","text":"","category":"section"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"It's possible to multiply matrices using @reduce,  but this is exceptionally inefficient, as it first broadcasts out a 3-tensor  before summing over one index:","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"julia> @pretty @reduce R[i,k] := sum(j) M[i,j] * N[j,k]\nbegin\n    local pelican = orient(N, (*, :, :))\n    R = dropdims(sum(@__dot__(M * pelican), dims=2), dims=2)\nend","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"The macro @matmul has the same syntax, but instead calls A*B. ","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"julia> A = rand(100,100); B = rand(100,100);\n\njulia> red(A,B) = @reduce R[i,k] := sum(j) A[i,j] * B[j,k];\n\njulia> mul(A,B) = @matmul P[i,k] := sum(j) A[i,j] * B[j,k];\n\njulia> using BenchmarkTools\n\njulia> @btime red($A, $B);\n  1.471 ms (25 allocations: 7.71 MiB)\n\njulia> @btime mul($A, $B);\n  33.489 μs (2 allocations: 78.20 KiB)","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"Of course you may just write A * B yourself, but in general @matmul will handle the same  reshaping, transposing, fixed indices, etc. steps as @reduce does.  Once all of that is done, however, the result must be a product like A * B in which the indices being summed over appear on both tensors.  <!– If there are more than two factors, then multiplication proceeds from the left, (A * B) * C.  But that has bugs! –>","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"To use the more flexible @reduce in cases like this,  when creating the large intermediate tensor will be expensive, the option lazy which creates a LazyArrays.BroadcastArray instead can help: ","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"julia> lazyred(A,B) = @reduce R[i,k] := sum(j) A[i,j] * B[j,k]  lazy;\n\njulia> @btime lazyred($A, $B);\n  223.172 μs (26 allocations: 78.95 KiB) ","category":"page"},{"location":"reduce/#","page":"Reduction","title":"Reduction","text":"More details on the next page. ","category":"page"},{"location":"basics/#Basics-1","page":"Basics","title":"Basics","text":"","category":"section"},{"location":"basics/#","page":"Basics","title":"Basics","text":"Install and set up:","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"(v1.1) pkg> add TensorCast","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> using TensorCast\n\njulia> V = [10,20,30];\n\njulia> M = collect(reshape(1:12, 3,4))\n3×4 Array{Int64,2}:\n 1  4  7  10\n 2  5  8  11\n 3  6  9  12","category":"page"},{"location":"basics/#Broadcasting-1","page":"Basics","title":"Broadcasting","text":"","category":"section"},{"location":"basics/#","page":"Basics","title":"Basics","text":"Here's a simple use of @cast, broadcasting addition over a matrix, a vector, and a scalar.  Using := makes a new array, after which the left and right will be equal for all possible  values of the indices:","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> @cast A[j,i] := M[i,j] + 10 * V[i]\n4×3 Array{Int64,2}:\n 101  202  303\n 104  205  306\n 107  208  309\n 110  211  312\n\njulia> all(A[j,i] == M[i,j] + 10 * V[i] for i=1:3, j=1:4)\ntrue","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"To make this happen, first M is transposed, and then the axis of V is re-oriented  to lie in the second dimension. The macro @pretty prints out what @cast produces:","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> @pretty @cast A[j,i] := M[i,j] + 10 * V[i]\nbegin\n    local pelican = PermuteDims(M)\n    local termite = orient(V, (*, :))\n    A = @__dot__(pelican + 10termite)\nend","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"Functions like orient take a code in which : represents an index which varies, while * is fixed. (Or fixed on a given slice, for sliceview below.) The reason to do this is that typeof(:) != typeof(*), and so this code is visible to the compiler, a trick I borrowed from JuliennedArrays.jl.","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"Of course @__dot__ is the full name of the broadcasting macro @.,  which simply produces A = pelican .+ 10 .* termite. And the animal names (in place of generated symbols) are from the indispensible MacroTools.jl.","category":"page"},{"location":"basics/#Slicing-1","page":"Basics","title":"Slicing","text":"","category":"section"},{"location":"basics/#","page":"Basics","title":"Basics","text":"Here are two ways to slice M into its columns, both of these produce S = sliceview(M, (:, *)) which is simply collect(eachcol(M)):","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> @cast S[j][i] := M[i,j]\n4-element Array{SubArray{Int64,1,Array{Int64,2},Tuple{Base.Slice{Base.OneTo{Int64}},Int64},true},1}:\n [1, 2, 3]\n [4, 5, 6]   \n [7, 8, 9]   \n [10, 11, 12]\n\njulia> all(S[j][i] == M[i,j] for i=1:3, j=1:4)\ntrue\n\njulia> @cast S2[j] := M[:,j]; \n\njulia> S == S2\ntrue","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"We can glue slices back together with the same notation, M2[i,j] := S[j][i].  Combining this with slicing from M[:,j] is a convenient way to perform mapslices operations:","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> @cast C[i,j] := cumsum(M[:,j])[i]\n3×4 Array{Int64,2}:\n 1   4   7  10\n 3   9  15  21\n 6  15  24  33\n\njulia> C == mapslices(cumsum, M, dims=1)\ntrue","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"This is both faster and more general than mapslices,  as it does not have to infer what shape the function produces:","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> using BenchmarkTools\n\njulia> f(M) = @cast [i,j] := cumsum(M[:,j])[i];\n\njulia> M10 = rand(10,1000);\n\njulia> @btime mapslices(cumsum, $M10, dims=1);\n  630.725 μs (7508 allocations: 400.28 KiB)\n\njulia> @btime f($M10);\n  64.056 μs (2006 allocations: 297.25 KiB)","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"It's also more readable, in less trivial examples:","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> using LinearAlgebra, Random; Random.seed!(42);\n\njulia> T = rand(3,3,5);\n\njulia> @cast E[i,n] := eigen(T[:,:,n]).values[i];\n\njulia> E == dropdims(mapslices(x -> eigen(x).values, T, dims=(1,2)), dims=2)\ntrue","category":"page"},{"location":"basics/#Fixed-indices-1","page":"Basics","title":"Fixed indices","text":"","category":"section"},{"location":"basics/#","page":"Basics","title":"Basics","text":"Sometimes it's useful to insert a trivial index into the output – for instance  to match the output of mapslices(x -> eigen(... just above:","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> @cast E3[i,_,n] := eigen(T[:,:,n]).values[i];\n\njulia> size(E3)\n(3, 1, 5)","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"Using _ on the right will demand that size(E3,2) == 1; but you can also fix indices to other values.  If these are variables not integers, they must be interpolated M[$row,j]  to distinguish them from index names:","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> col = 3;\n\njulia> @cast O[i,j] := (M[i,1], M[j,$col])\n3×3 Array{Tuple{Int64,Int64},2}:\n (1, 7)  (1, 8)  (1, 9)\n (2, 7)  (2, 8)  (2, 9)\n (3, 7)  (3, 8)  (3, 9)","category":"page"},{"location":"basics/#Reshaping-1","page":"Basics","title":"Reshaping","text":"","category":"section"},{"location":"basics/#","page":"Basics","title":"Basics","text":"Sometimes it's useful to combine two (or more) indices into one,  which may be written  either i⊗j or i\\j or (i,j).  The simplest version of this is precisely what the built-in function kron does:","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> W = 1 ./ [10,20,30,40];\n\njulia> @cast K[_, i⊗j] := V[i] * W[j]\n1×12 Array{Float64,2}:\n 1.0  2.0  3.0  0.5  1.0  1.5  0.333333  0.666667  1.0  0.25  0.5  0.75\n\njulia> K == kron(W, V)'\ntrue\n\njulia> all(K[i + 3(j-1)] == V[i] * W[j] for i=1:3, j=1:4)\ntrue\n\njulia> Base.kron(A::Array{T,3}, X::Array{T′,3}) where {T,T′} =    # extend kron to 3-tensors\n           @cast [x⊗a, y⊗b, z⊗c] := A[a,b,c] * X[x,y,z]","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"If an array on the right has a combined index, then it may be ambiguous how to divide up its range. You can resolve this by providing explicit ranges, after the main expression: ","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> @cast A[i,j] := collect(1:12)[i⊗j]  i:2\n2×6 Array{Int64,2}:\n 1  3  5  7   9  11\n 2  4  6  8  10  12\n\njulia> @cast A[i,j] := collect(1:12)[i⊗j]  i:4, j:3\n4×3 Array{Int64,2}:\n 1  5   9\n 2  6  10\n 3  7  11\n 4  8  12","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"Writing into a given array with = instead of := will also remove ambiguities,  as size(A) is known:","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> @cast A[i,j] = 10 * collect(1:12)[i⊗j];","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"Aside, note that providing explicit ranges will also turn on checks of the input, for example:","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> @pretty @cast W[i] := V[i]^2  i:3 \nbegin\n    @assert_ 3 == size(V, 1) \"range of index i must agree\"\n    @assert_ ndims(V) == 1 \"expected a 1-tensor V[i]\"\n    W = @__dot__(V ^ 2)\nend","category":"page"},{"location":"basics/#Glue-and-reshape-1","page":"Basics","title":"Glue & reshape","text":"","category":"section"},{"location":"basics/#","page":"Basics","title":"Basics","text":"As a less trivial example of these combined indices, here is one way to combine a list of matrices into one large grid. Notice that x varies faster than i, and so on – the linear order of index x⊗i agrees with that of two indices x,i. ","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> list = [ i .* ones(2,2) for i=1:8 ];\n\njulia> @cast mat[x\\i, y\\j] := Int(list[i\\j][x,y])  i:2\n4×8 Array{Int64,2}:\n 1  1  3  3  5  5  7  7\n 1  1  3  3  5  5  7  7\n 2  2  4  4  6  6  8  8\n 2  2  4  4  6  6  8  8\n\njulia> vec(mat) == @cast [xi\\yj] := mat[xi, yj]\ntrue\n\njulia> mat == Int.(hvcat((4,4), transpose(reshape(list,2,4))...))\ntrue","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"Alternatively, this reshapes each matrix to a vector, and makes them columns of the output:","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> @cast colwise[x\\y,i] := Int(list[i][x,y])\n4×8 Array{Int64,2}:\n 1  2  3  4  5  6  7  8\n 1  2  3  4  5  6  7  8\n 1  2  3  4  5  6  7  8\n 1  2  3  4  5  6  7  8\n\njulia> colwise == Int.(reduce(hcat, vec.(list)))\ntrue","category":"page"},{"location":"basics/#Reverse-and-shuffle-1","page":"Basics","title":"Reverse & shuffle","text":"","category":"section"},{"location":"basics/#","page":"Basics","title":"Basics","text":"A minus in front of an index will reverse that direction, and a tilde will shuffle it.  Both create views, which you may explicitly collect using |=:","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> @cast M2[i,j] := M[i,-j]\n3×4 view(::Array{Int64,2}, :, 4:-1:1) with eltype Int64:\n 10  7  4  1\n 11  8  5  2\n 12  9  6  3\n\njulia> using Random; Random.seed!(42); \n\njulia> @cast M3[i,j] |= M[i,~j]\n3×4 Array{Int64,2}:\n 7  4  1  10\n 8  5  2  11\n 9  6  3  12","category":"page"},{"location":"basics/#Primes-'-1","page":"Basics","title":"Primes '","text":"","category":"section"},{"location":"basics/#","page":"Basics","title":"Basics","text":"Acting on indices, A[i'] is normalised to A[i′] unicode \\prime (which looks identical in some fonts). Acting on elements, C[i,j]' means adjoint.(C), elementwise complex conjugate, equivalent to (C')[j,i]. If the elements are matrices, as in C[:,:,k]', then  adjoint is conjugate transpose.","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> @cast C[i,i'] := (1:4)[i⊗i′] + im  (i:2, i′:2)\n2×2 Array{Complex{Int64},2}:\n 1+1im  3+1im\n 2+1im  4+1im\n\njulia> @cast [i,j] := C[i,j]'\n2×2 Array{Complex{Int64},2}:\n 1-1im  3-1im\n 2-1im  4-1im\n\njulia> C' == @cast [j,i] := C[i,j]'\ntrue","category":"page"},{"location":"basics/#Arrays-of-indices-or-functions-1","page":"Basics","title":"Arrays of indices or functions","text":"","category":"section"},{"location":"basics/#","page":"Basics","title":"Basics","text":"Besides arrays of numbers (and arrays of arrays) you can also broadcast an array of functions, which is done by calling apply(f,xs...) = f(xs...): ","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> funs = [identity, sqrt];\n\njulia> @cast applied[i,j] := funs[i](V[j])\n2×3 Array{Real,2}:\n 10        20        30      \n  3.16228   4.47214   5.47723\n\njulia> @pretty @cast applied[i,j] := funs[i](V[j])\nbegin\n    local pelican = orient(V, (*, :))\n    applied = @__dot__(apply(funs, pelican))\nend","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"You can also index one array using another, this example is just view(M, :, ind):","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> ind = [1,1,2,2,4];\n\njulia> @cast [i,j] := M[i,ind[j]]\n3×5 view(::Array{Int64,2}, :, [1, 1, 2, 2, 4]) with eltype Int64:\n 1  1  4  4  10\n 2  2  5  5  11\n 3  3  6  6  12","category":"page"},{"location":"basics/#Repeated-indices-1","page":"Basics","title":"Repeated indices","text":"","category":"section"},{"location":"basics/#","page":"Basics","title":"Basics","text":"The only situation in which repeated indices are allowed is when they either  extract the diagonal of a matrix, or create a diagonal matrix:","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> @cast D[i] |= C[i,i]\n2-element Array{Complex{Int64},1}:\n 1 + 1im\n 4 + 1im\n\njulia> D2 = @cast [i,i] := V[i]\n3×3 Diagonal{Int64,Array{Int64,1}}:\n 10   ⋅   ⋅\n  ⋅  20   ⋅\n  ⋅   ⋅  30","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"All indices appearing on the left must also appear on the right.  There is no implicit sum over repeated indices on different tensors. To sum over things, you need @reduce or @matmul. ","category":"page"},{"location":"options/#Options-1","page":"Options","title":"Options","text":"","category":"section"},{"location":"options/#","page":"Options","title":"Options","text":"Expressions with = write into an existing array,  while those with := do not. This is the same notation as  TensorOperations.jl and Einsum.jl.  But unlike those packages, sometimes the result of @cast is a view of the original, for instance  @cast A[i,j] := B[j,i] gives A = transpose(B). You can forbid this, and insist on a copy,  by writing |= instead (or passing the option collect). ","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"Various other options can be given after the main expression. assert turns on explicit size checks,  and ranges like i:3 specify the size in that direction (sometimes this is necessary to specify the shape). Adding these to the example above: ","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"@pretty @cast A[(i,j)] = B[i,j]  i:3, assert\n# begin\n#     @assert_ ndims(B) == 2 \"expected a 2-tensor B[i, j]\"\n#     @assert_ 3 == size(B, 1) \"range of index i must agree\"\n#     @assert_ ndims(A) == 1 \"expected a 1-tensor A[(i, j)]\"\n#     copyto!(A, B)\n# end","category":"page"},{"location":"options/#Ways-of-slicing-1","page":"Options","title":"Ways of slicing","text":"","category":"section"},{"location":"options/#","page":"Options","title":"Options","text":"The default way of slicing creates an array of views,  but if you use |= instead then you get copies: ","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"M = rand(1:99, 3,4)\n\n@cast S[k][i] := M[i,k]             # collect(eachcol(M)) ≈ [ view(M,:,k) for k=1:4 ]\n@cast S[k][i] |= M[i,k]             # [ M[:,k] for k=1:4 ]; using |= demands a copy","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"The default way of un-slicing is reduce(hcat, ...), which creates a new array.  But there are other options, controlled by keywords after the expression:","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"@cast A[i,k] := S[k][i]             # A = reduce(hcat, B)\n@cast A[i,k] := S[k][i]  cat        # A = hcat(B...); often slow\n@cast A[i,k] := S[k][i]  lazy       # A = LazyStack.stack(B)\n\nsize(A) == (3, 4) # true","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"The option lazy uses LazyStack.jl to create a view of the original vectors. ","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"Another kind of slices are provided by StaticArrays.jl, in which a Vector of SVectors is just a different interpretation of the same memory as a Matrix.  By another slight abuse of notation, such slices are written here as curly brackets:","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"@cast S[k]{i} := M[i,k]  i:3        # S = reinterpret(SVector{3,Int}, vec(M)) \n@cast S[k] := M{:3, k}              # equivalent\n\n@cast R[k,i] := S[k]{i}             # such slices can be reinterpreted back again","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"Both S and R here are views of the original matrix M.  When creating such slices, their size ought to be provided, either as a literal integer or  through the types. Note that you may also write S[k]{i:3} = .... ","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"The second notation (with M{:,k}) is useful for mapslices. Continuing the example: ","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"M10 = rand(10,1000); \nmapslices(cumsum, M10, dims=1)          # 630 μs using @btime\n@cast [i,j] := cumsum(M10[:,j])[i]      #  64 μs\n@cast [i,j] := cumsum(M10{:10,j})[i]    #  38 μs","category":"page"},{"location":"options/#Better-broadcasting-1","page":"Options","title":"Better broadcasting","text":"","category":"section"},{"location":"options/#","page":"Options","title":"Options","text":"When broadcasting and then summing over some directions, it can be faster to avoid creating the  entire array, then throwing it away. This can be done with the package  LazyArrays.jl which has a lazy BroadcastArray.  In the following example, the product V .* V' .* V3 contains about 1GB of data,  the writing of which is avoided by giving the option lazy: ","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"V = rand(500); V3 = reshape(V,1,1,:);\n\n@time @reduce W[i] := sum(j,k) V[i]*V[j]*V[k];        # 0.6 seconds, 950 MB\n@time @reduce W[i] := sum(j,k) V[i]*V[j]*V[k]  lazy;  # 0.025 s, 5 KB","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"The package Strided.jl can apply multi-threading to  broadcasting, and some other magic. You can enable it with the option strided, like this: ","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"using Strided # and export JULIA_NUM_THREADS = 4 before starting\nA = randn(4000,4000); B = similar(A);\n\n@time @cast B[i,j] = (A[i,j] + A[j,i])/2;             # 0.12 seconds\n@time @cast B[i,j] = (A[i,j] + A[j,i])/2 strided;     # 0.025 seconds","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"The package LoopVectorization.jl provides  a macro @avx which modifies broadcasting to use vectorised instructions.  This is new and does not work for all broadcasting operations!  But it can be used via the option avx:","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"using LoopVectorization, BenchmarkTools\nC = randn(40,40); \n\nD = @btime @cast [i,j] := exp($C[i,j]);        # 13 μs\nD′ = @btime @cast [i,j] := exp($C[i,j]) avx;   #  3 μs\nD ≈ D′","category":"page"},{"location":"options/#Less-lazy-1","page":"Options","title":"Less lazy","text":"","category":"section"},{"location":"options/#","page":"Options","title":"Options","text":"To disable the default use of PermutedDimsArray etc, give the option nolazy: ","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"@pretty @cast Z[y,x] := M[x,-y]  nolazy\n# Z = reverse(permutedims(M), dims=1)\n\n@pretty @cast Z[y,x] := M[x,-y] \n# Z = Reverse{1}(PermuteDims(M))","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"This also controls how the extraction of diagonal elements and creation of diagonal matrices are done:","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"@pretty @cast M[i,i] := A[i,i]  nolazy\n# M = diagm(0 => diag(A))\n\n@pretty @cast D[i,i] := A[i,i]\n# D = Diagonal(diagview(A))\n\n@pretty @cast M[i,i] = A[i,i]  nolazy  # into diagonal of existing matrix M\n# copyto!(diagview(M), diag(A)); M","category":"page"},{"location":"options/#","page":"Options","title":"Options","text":"Here TensorCast.Reverse{1}(B) creates a view with reverse(axes(B,1)).  TensorCast.PermuteDims(M) is transpose(M) on a matrix of numbers, else PermutedDimsArray. And TensorCast.diagview(A) is just view(A, diagind(A)).","category":"page"},{"location":"options/#Gradients-1","page":"Options","title":"Gradients","text":"","category":"section"},{"location":"options/#","page":"Options","title":"Options","text":"I moved some code here from SliceMap.jl, which I should describe!","category":"page"},{"location":"#TensorCast.jl-1","page":"Home","title":"TensorCast.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"This package lets you write complicated formulae in index notation, which are turned into Julia's usual broadcasting, permuting, slicing, and reducing operations. ","category":"page"},{"location":"#Documentation-1","page":"Home","title":"Documentation","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"The pages are:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Use of @cast for broadcasting, and slicing\n@reduce and @matmul, for summing over some directions\nOptions: StaticArrays, LazyArrays, Strided\nDocstrings, for all details.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"These refer to v0.2, which is currently #master,  see the readme for a list of what's changed.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"What will show up on pkg.julialang.org once this works? Not sure.","category":"page"},{"location":"docstrings/#Macros-1","page":"Docstrings","title":"Macros","text":"","category":"section"},{"location":"docstrings/#","page":"Docstrings","title":"Docstrings","text":"CurrentModule = TensorCast","category":"page"},{"location":"docstrings/#","page":"Docstrings","title":"Docstrings","text":"@cast(ex)","category":"page"},{"location":"docstrings/#TensorCast.@cast-Tuple{Any}","page":"Docstrings","title":"TensorCast.@cast","text":"@cast Z[i,j,...] := f(A[i,j,...], B[j,k,...])  options\n\nMacro for broadcasting, reshaping, and slicing of arrays in index notation. Understands the following things:\n\nA[i,j,k] is a three-tensor with these indices.\nB[(i,j),k] is the same thing, reshaped to a matrix. Its first axis (the bracket) is indexed by n = i + (j-1) * N where i ∈ 1:N. This may also be written B[i\\j,k] or B[i⊗j,k].\nC[k][i,j] is a vector of matrices, either created by slicing (if on the left) or implying glueing into (if on the right) a 3-tensor A[i,j,k].\nD[j,k]{i} is an ordinary matrix of SVectors, which may be reinterpreted from A[i,j,k].\nE[i,_,k] has two nontrivial dimensions, and size(E,2)==1. On the right hand side (or when writing to an existing array) you may also write E[i,3,k] meaning view(E, :,3,:), or E[i,$c,j] to use a variable c.\nf(x)[i,j,k] is allowed; f(x) must return a 3-tensor (and will be evaluated only once).\ng(H[:,k])[i,j] is a generalised mapslices, with g mapping columns of H   to matrices, which are glued into a 3-tensor A[i,j,k].\nh(I[i], J[j])[k] expects an h which maps two scalars to a vector, which gets broadcasted h.(I,J') then glued into to a 3-tensor.\nK[i,j]' conjugates each element, equivalent to K'[j,i] which is the conjugate-transpose of the matrix.\nM[i,i] means diag(M)[i], but only for matrices: N[i,i,k] is an error.\nP[i,i'] is normalised to P[i,i′] with unicode \\prime.\nR[i,-j,k] means roughly reverse(R, dims=2), and Q[i,-j,k] similar with shuffle.\nS[i,T[j,k]] is the 3-tensor S[:,T] created by indexing a matrix S with T, where these integers are all(1 .<= T .<= size(S,2)).\n\nThe left and right hand sides must have all the same indices, and the only repeated index allowed is M[i,i], which is a diagonal not a trace. See @reduce and @matmul for related macros which can sum over things.\n\nIf a function of one or more tensors appears on the right hand side, then this represents a broadcasting operation, and the necessary re-orientations of axes are automatically inserted.\n\nThe following actions are possible:\n\n= writes into an existing array, overwriting its contents, while += adds (precisely Z .= Z .+ ...) and *= multiplies.\n:= creates a new array. This need not be named: Z = @cast [i,j,k] := ... is allowed.\n|= insists that this is a copy, not a view.\n\nRe-ordering of indices Z[k,j,i] is done lazily with PermutedDimsArray(A, ...). Reversing of an axis F[i,-j,k] is also done lazily, by Reverse{2}(F) which makes a view. Using |= (or broadcasting) will produce a simple Array.\n\nOptions can be specified at the end (if several, separated by , i.e. options::Tuple)\n\ni:3 supplies the range of index i. Variables and functions like j:Nj, k:length(K) are allowed.\nassert will turn on explicit dimension checks of the input. (Providing any ranges will also turn these on.)\ncat will glue slices by things like hcat(A...) instead of the default reduce(hcat, A), and lazy will instead make a LazyStack.Stacked container.\nnolazy disables PermutedDimsArray and Reverse in favour of permutedims and reverse, and Diagonal in favour of diagm for Z[i,i] output.\nstrided will place @strided in front of broadcasting operations, and use @strided permutedims(A, ...) instead of PermutedDimsArray(A, ...). For this you need using Strided to load that package.\navx will place @avx in front of broadcasting operations, which needs using LoopVectorization to load that package.\n\nTo create static slices D[k]{i,j} you should give all slice dimensions explicitly. You may write D[k]{i:2,j:2} to specify Size(2,2) slices. They are made most cleanly from the first indices of the input, i.e. this D from A[i,j,k]. The notation A{:,:,k} will only work in this order, and writing A{:2,:2,k} provides the sizes.\n\n\n\n\n\n","category":"macro"},{"location":"docstrings/#","page":"Docstrings","title":"Docstrings","text":"@reduce(ex)","category":"page"},{"location":"docstrings/#TensorCast.@reduce-Tuple{Any}","page":"Docstrings","title":"TensorCast.@reduce","text":"@reduce A[i] := sum(j,k) B[i,j,k]             # A = vec(sum(B, dims=(2,3)))\n@reduce A[i] := prod(j) B[i] + ε * C[i,j]     # A = vec(prod(B .+ ε .* C, dims=2))\n@reduce A[i] = sum(j) exp( C[i,j] / D[j] )    # sum!(A, exp.(C ./ D') )\n\nTensor reduction macro:\n\nThe reduction function can be anything which works like sum(B, dims=(1,3)), for instance prod and maximum and Statistics.mean.\nIn-place operations Z[j] = sum(... will construct the banged version of the given function's name, which must work like sum!(Z, A).\nThe tensors can be anything that @cast understands, including gluing of slices B[i,k][j] and reshaping B[i\\j,k]. See ? @cast for the complete list.\nIf there is a function of one or more tensors on the right, then this is a broadcasting operation.\nIndex ranges may be given afterwards (as for @cast) or inside the reduction sum(i:3, k:4).\nAll indices appearing on the right must appear either within sum(...) etc, or on the left.\n\nF = @reduce sum(i,j)  B[i] + γ * D[j]         # sum(B .+ γ .* D')\n@reduce G[] := sum(i,j)  B[i] + γ * D[j]      # F == G[]\n\nComplete reduction to a scalar output F, or a zero-dim array G. G[] involves sum(A, dims=(1,2)) rather than sum(A).\n\n@reduce Z[k] := sum(i,j) A[i] * B[j] * C[k]  lazy, i:N, j:N, k:N\n\nThe option lazy replaces the broadcast expression with a BroadcastArray, to avoid materializeing the entire array before summing. In the example this is of size N^3.\n\nThe option strided will place @strided in front of the broadcasting operation. You need using Strided for this to work.\n\n@reduce sum(i) A[i] * log(@reduce [i] := sum(j) A[j] * exp(B[i,j]))\n@cast W[i] := A[i] * exp(- @reduce S[i] = sum(j) exp(B[i,j]) lazy)\n\nRecursion like this is allowed, inside either @cast or @reduce. The intermediate array need not have a name, like [i], unless writing into an existing array, like S[i] here.\n\n\n\n\n\n","category":"macro"},{"location":"docstrings/#","page":"Docstrings","title":"Docstrings","text":"@matmul(ex)","category":"page"},{"location":"docstrings/#TensorCast.@matmul-Tuple{Any}","page":"Docstrings","title":"TensorCast.@matmul","text":"@matmul M[a,c] := sum(b)  A[a,b] * B[b,c]\n\nMatrix multiplication macro. Uses the same syntax as @reduce, but instead of broadcasting the expression on the right out to a 3-tensor before summing it along one dimension, this calls * or mul!, which is usually much faster. But it only works on expressions of suitable form.\n\nWith more than two tensors on the right, it proceeds left to right, and each summed index must appear on two tensors, probably neighbours.\n\nNote that unlike @einsum and @tensor, you must explicitly specify what indices to sum over. Both this macro and @reduce could in principal infer this, and perhaps I will add that later... but then I find myself commenting # sum_μ anyway.\n\n@matmul Z[a⊗b,z] = sum(i,j,k)  D[i,a][j] * E[i⊗j,_,k,b] * F[z,3,k]\n\nEach tensor will be pre-processed exactly as for @cast / @reduce, here glueing slices of D together, reshaping E, and taking a view of F. Once this is done, the right hand side must be of the form (tensor) * (tensor) * ..., which becomes mul!(ZZ, (DD * EE), FF).\n\n@reduce V[i] := sum(k) W[k] * exp(@matmul [i,k] := sum(j) A[i,j] * B[j,k])\n\nYou should be able to use this within the other macros, as shown.\n\n\n\n\n\n","category":"macro"},{"location":"docstrings/#","page":"Docstrings","title":"Docstrings","text":"@pretty(ex)","category":"page"},{"location":"docstrings/#TensorCast.@pretty-Tuple{Any}","page":"Docstrings","title":"TensorCast.@pretty","text":"@pretty @cast A[...] := B[...]\n\nPrints an approximately equivalent expression with the macro expanded. Compared to @macroexpand1, generated symbols are replaced with animal names (from MacroTools), comments are deleted, module names are removed from functions, and the final expression is fed to println().\n\nTo copy and run the printed expression, you may need various functions which aren't exported. Try something like using TensorCast: orient, star, rview, @assert_, red_glue, sliceview\n\n\n\n\n\n","category":"macro"},{"location":"docstrings/#String-macros-1","page":"Docstrings","title":"String macros","text":"","category":"section"},{"location":"docstrings/#","page":"Docstrings","title":"Docstrings","text":"These provide an alternative... but don't cover quite everything:","category":"page"},{"location":"docstrings/#","page":"Docstrings","title":"Docstrings","text":"@cast_str","category":"page"},{"location":"docstrings/#TensorCast.@cast_str","page":"Docstrings","title":"TensorCast.@cast_str","text":"cast\" Z_ij := A_i + B_j \"\n\nString macro version of @cast, which translates things like \"A_ijk\" == A[i,j,k]. Indices should be single letters, except for primes, for example:\n\njulia> @pretty cast\" X_αβ' = A_α3β' * log(B_β'$k)\"\n# @cast  X[α,β′] = A[α,3,β′] * log(B[β′,$k])\nbegin\n    local kangaroo = view(A, :, 3, :)\n    local turtle = transpose(view(B, :, k))\n    X .= @. kangaroo * log(turtle)\nend\n\nUnderscores (trivial dimensions) and colons (for mapslices) are also understood:\n\njulia> @pretty cast\" Y_ijk := f(A_j:k)_i + B_i_k + (C_k)_i\"\n# @cast  Y[i,j,k] := f(A[j,:,k])[i] + B[i,_,k] + C[k][i]\n\nOperators := and = work as usual, as do options. There are similar macros reduce\"Z_i = Σ_j A_ij\" and matmul\"Z_ik = Σ_j A_ij * B_jk\".\n\n\n\n\n\n","category":"macro"},{"location":"docstrings/#","page":"Docstrings","title":"Docstrings","text":"@reduce_str","category":"page"},{"location":"docstrings/#TensorCast.@reduce_str","page":"Docstrings","title":"TensorCast.@reduce_str","text":"reduce\" Z_i := sum_j A_ij + B_j \"\n\nString macro version of @reduce. Indices should be single letters, except for primes, and constants. You may write \\Sigma Σ for sum (and \\Pi Π for prod):\n\njulia> @pretty reduce\" W_i = Σ_i' A_ii' / B_i'3^2  lazy\"\n# @reduce  W[i] = sum(i′) A[i,i′] / B[i′,3]^2  lazy\nbegin\n    local mallard = orient(view(B, :, 3), (*, :))\n    sum!(W, @__dot__(lazy(A / mallard ^ 2)))\nend\n\n\n\n\n\n","category":"macro"},{"location":"docstrings/#Functions-1","page":"Docstrings","title":"Functions","text":"","category":"section"},{"location":"docstrings/#","page":"Docstrings","title":"Docstrings","text":"These are not exported, but are called by the macros above,  and visible in what @pretty prints out. ","category":"page"},{"location":"docstrings/#Reshaping-and-views-1","page":"Docstrings","title":"Reshaping & views","text":"","category":"section"},{"location":"docstrings/#","page":"Docstrings","title":"Docstrings","text":"TensorCast.orient","category":"page"},{"location":"docstrings/#TensorCast.orient","page":"Docstrings","title":"TensorCast.orient","text":"B = orient(A, code)\n\nReshapes A such that its nontrivial axes lie in the directions where code contains a :, by inserting axes on which size(B, d) == 1 as needed. Throws an error if ndims(A) != length(code).\n\nWhen acting on A::Transpose, A::PermutedDimsArray etc, it will collect(A) first, because reshaping these is very slow.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#","page":"Docstrings","title":"Docstrings","text":"TensorCast.PermuteDims","category":"page"},{"location":"docstrings/#TensorCast.PermuteDims","page":"Docstrings","title":"TensorCast.PermuteDims","text":"PermuteDims(A::Matrix)\nPermuteDims(A::Vector)\n\nLazy like transpose, but not recursive: calls PermutedDimsArray unless eltype(A) <: Number.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#","page":"Docstrings","title":"Docstrings","text":"TensorCast.rview","category":"page"},{"location":"docstrings/#TensorCast.rview","page":"Docstrings","title":"TensorCast.rview","text":"rview(A, :,1,:) ≈ (@assert size(A,2)==1; view(A, :,1,:))\n\nThis simply reshapes A so as to remove a trivial dimension where indicated. Throws an error if size of A is not 1 in the indicated dimension.\n\nWill fail silently if given a (:,3,:) or (:,$c,:), for which needview!(code) = true, so the macro should catch such cases.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#","page":"Docstrings","title":"Docstrings","text":"TensorCast.diagview","category":"page"},{"location":"docstrings/#TensorCast.diagview","page":"Docstrings","title":"TensorCast.diagview","text":"diagview(M) = view(M, diagind(M))\n\nLike diag(M) but makes a view.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#Slicing-and-glueing-1","page":"Docstrings","title":"Slicing & glueing","text":"","category":"section"},{"location":"docstrings/#","page":"Docstrings","title":"Docstrings","text":"TensorCast.sliceview","category":"page"},{"location":"docstrings/#TensorCast.sliceview","page":"Docstrings","title":"TensorCast.sliceview","text":"sliceview(A, code)\nslicecopy(A, code)\n\nSlice array A according to code, a tuple of length ndims(A), in which : indicates a dimension of the slices, and * a dimension separating them. For example if code = (:,*,:) then slices are either view(A, :,i,:) or A[:,i,:] with i=1:size(A,2).\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#","page":"Docstrings","title":"Docstrings","text":"TensorCast.glue","category":"page"},{"location":"docstrings/#TensorCast.glue","page":"Docstrings","title":"TensorCast.glue","text":"glue!(B, A, code)\ncopy_glue(A, code) = glue!(Array{T}(...), A, code)\n\nCopy the contents of an array of arrays into one larger array, un-doing sliceview / slicecopy with the same code. This function can handle arbitrary codes. (Also called stack or align elsewhere.)\n\ncat_glue(A, code)\nred_glue(A, code)\n\nThe same result, but calling either things like hcat(A...) or things like reduce(hcat, A). The code must be sorted like (:,:,:,*,*), except that (*,:) is allowed.\n\nglue(A)\nglue(A, code)\n\nIf code is omitted, the default is like (:,:,*,*) with ndims(first(A)) colons first, then ndims(A) stars. If the inner arrays are StaticArrays (and the code is sorted) then it calls static_glue. Otherwise it will call red_glue, unless code is unsuitable for that, in which case copy_glue.\n\n\n\n\n\n","category":"function"}]
}
